{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Erfzt4E40vKd"
      },
      "source": [
        "#STUDENT DETAILS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adEfyTWI0y1I"
      },
      "source": [
        "STUDENT NAME : Aayush Thakar\n",
        "\n",
        "STUDENT ID : 24041785"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Nar64Oq04Zx"
      },
      "source": [
        "#Why Reinforcement Learning is the ML paradigm of choice for this task?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Wkyun070-MZ"
      },
      "source": [
        "Reinforcement Learning (RL) is the best approach because it is all about training agent to make decisions in real world over time in environmental condition that is constantly changing. In the game of Atari  **[ALE/DemonAttack-v5]** , the agent is not simply reacting in the moment, but has to anticipate, train from its action to action and adjust in response to that what works. Every action it carries out such as evading a hit or firing a demon provides it an opportunity of feedback type rewards. As it plays more, it acquires an understanding on how to improve on gameplay and also raise the score.\n",
        "\n",
        "Unlike supervised learning which trains on labelled data, RL does not require labels. However, the agent does not learn what to do from a knowledge base but learns it because he tries out things and sees what ensues. That is why RL, particularly Deep Q-Networks (DQNs), is the method of choice for beating games like this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8TvqJH30qcF"
      },
      "source": [
        "INSTALLING NECESSARY PACKAGES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5a4I3vHa0qcG",
        "outputId": "1cc37bcf-aaeb-4a09-b0f4-8f86bf50bf9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in c:\\users\\aayus\\dqn\\.conda\\lib\\site-packages (2.4.1+cu118)Note: you may need to restart the kernel to use updated packages.\n",
            "\n",
            "Requirement already satisfied: torchvision in c:\\users\\aayus\\dqn\\.conda\\lib\\site-packages (0.19.1)\n",
            "Requirement already satisfied: numpy in c:\\users\\aayus\\dqn\\.conda\\lib\\site-packages (1.24.4)\n",
            "Requirement already satisfied: opencv-python in c:\\users\\aayus\\dqn\\.conda\\lib\\site-packages (4.11.0.86)\n",
            "Requirement already satisfied: gym[atari] in c:\\users\\aayus\\dqn\\.conda\\lib\\site-packages (0.26.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\aayus\\dqn\\.conda\\lib\\site-packages (from gym[atari]) (3.1.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in c:\\users\\aayus\\dqn\\.conda\\lib\\site-packages (from gym[atari]) (0.0.8)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in c:\\users\\aayus\\dqn\\.conda\\lib\\site-packages (from gym[atari]) (7.0.1)\n",
            "Requirement already satisfied: ale-py~=0.8.0 in c:\\users\\aayus\\dqn\\.conda\\lib\\site-packages (from gym[atari]) (0.8.1)\n",
            "Requirement already satisfied: autorom~=0.4.2 in c:\\users\\aayus\\dqn\\.conda\\lib\\site-packages (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license]) (0.4.2)\n",
            "Requirement already satisfied: filelock in c:\\users\\aayus\\dqn\\.conda\\lib\\site-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\aayus\\dqn\\.conda\\lib\\site-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in c:\\users\\aayus\\dqn\\.conda\\lib\\site-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in c:\\users\\aayus\\dqn\\.conda\\lib\\site-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\aayus\\dqn\\.conda\\lib\\site-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in c:\\users\\aayus\\dqn\\.conda\\lib\\site-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\aayus\\dqn\\.conda\\lib\\site-packages (from torchvision) (10.4.0)\n",
            "Requirement already satisfied: importlib-resources in c:\\users\\aayus\\dqn\\.conda\\lib\\site-packages (from ale-py~=0.8.0->gym[atari]) (6.4.5)\n",
            "Requirement already satisfied: click in c:\\users\\aayus\\dqn\\.conda\\lib\\site-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license]) (8.1.8)\n",
            "Requirement already satisfied: requests in c:\\users\\aayus\\dqn\\.conda\\lib\\site-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license]) (2.32.3)\n",
            "Requirement already satisfied: tqdm in c:\\users\\aayus\\dqn\\.conda\\lib\\site-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license]) (4.67.1)\n",
            "Requirement already satisfied: AutoROM.accept-rom-license in c:\\users\\aayus\\dqn\\.conda\\lib\\site-packages (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license]) (0.6.1)\n",
            "Requirement already satisfied: zipp>=0.5 in c:\\users\\aayus\\dqn\\.conda\\lib\\site-packages (from importlib-metadata>=4.8.0->gym[atari]) (3.20.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\aayus\\dqn\\.conda\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\aayus\\dqn\\.conda\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\aayus\\dqn\\.conda\\lib\\site-packages (from click->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license]) (0.4.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\aayus\\dqn\\.conda\\lib\\site-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license]) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aayus\\dqn\\.conda\\lib\\site-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aayus\\dqn\\.conda\\lib\\site-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license]) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aayus\\dqn\\.conda\\lib\\site-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license]) (2025.1.31)\n"
          ]
        }
      ],
      "source": [
        "%pip install gym[atari] gym[accept-rom-license] torch torchvision numpy opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "e5mTLwHu0qcG",
        "outputId": "0800b653-c575-4ed7-cab0-4a3c26d6176c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: matplotlib in c:\\users\\aayus\\dqn\\.conda\\lib\\site-packages (3.7.5)Note: you may need to restart the kernel to use updated packages.\n",
            "\n",
            "Requirement already satisfied: tqdm in c:\\users\\aayus\\dqn\\.conda\\lib\\site-packages (4.67.1)\n",
            "Requirement already satisfied: ale-py in c:\\users\\aayus\\dqn\\.conda\\lib\\site-packages (0.8.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\aayus\\dqn\\.conda\\lib\\site-packages (from matplotlib) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\aayus\\dqn\\.conda\\lib\\site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\aayus\\dqn\\.conda\\lib\\site-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\aayus\\dqn\\.conda\\lib\\site-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: numpy<2,>=1.20 in c:\\users\\aayus\\dqn\\.conda\\lib\\site-packages (from matplotlib) (1.24.4)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\aayus\\dqn\\.conda\\lib\\site-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\aayus\\dqn\\.conda\\lib\\site-packages (from matplotlib) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\aayus\\dqn\\.conda\\lib\\site-packages (from matplotlib) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\aayus\\dqn\\.conda\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\aayus\\dqn\\.conda\\lib\\site-packages (from matplotlib) (6.4.5)\n",
            "Requirement already satisfied: colorama in c:\\users\\aayus\\dqn\\.conda\\lib\\site-packages (from tqdm) (0.4.6)\n",
            "Requirement already satisfied: importlib-metadata>=4.10.0 in c:\\users\\aayus\\dqn\\.conda\\lib\\site-packages (from ale-py) (7.0.1)\n",
            "Requirement already satisfied: typing-extensions in c:\\users\\aayus\\dqn\\.conda\\lib\\site-packages (from ale-py) (4.11.0)\n",
            "Requirement already satisfied: zipp>=0.5 in c:\\users\\aayus\\dqn\\.conda\\lib\\site-packages (from importlib-metadata>=4.10.0->ale-py) (3.20.2)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\aayus\\dqn\\.conda\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "%pip install matplotlib tqdm ale-py tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "gR6Y44pU0qcH",
        "outputId": "78307928-0fcb-4eec-f327-5a39e93479ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118Note: you may need to restart the kernel to use updated packages.\n",
            "\n",
            "Requirement already satisfied: torch in c:\\users\\aayus\\dqn\\.conda\\lib\\site-packages (2.4.1+cu118)\n",
            "Requirement already satisfied: torchvision in c:\\users\\aayus\\dqn\\.conda\\lib\\site-packages (0.19.1)\n",
            "Requirement already satisfied: torchaudio in c:\\users\\aayus\\dqn\\.conda\\lib\\site-packages (2.4.1+cu118)\n",
            "Requirement already satisfied: filelock in c:\\users\\aayus\\dqn\\.conda\\lib\\site-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\aayus\\dqn\\.conda\\lib\\site-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in c:\\users\\aayus\\dqn\\.conda\\lib\\site-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in c:\\users\\aayus\\dqn\\.conda\\lib\\site-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\aayus\\dqn\\.conda\\lib\\site-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in c:\\users\\aayus\\dqn\\.conda\\lib\\site-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\aayus\\dqn\\.conda\\lib\\site-packages (from torchvision) (1.24.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\aayus\\dqn\\.conda\\lib\\site-packages (from torchvision) (10.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\aayus\\dqn\\.conda\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\aayus\\dqn\\.conda\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UL6DoxQN0qcH"
      },
      "source": [
        "IMPORTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "UukjvqcY0qcH"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import random\n",
        "from collections import deque\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AynxXSYi0qcH"
      },
      "source": [
        "NVIDIA GPU - Geforce GTX 1650"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jyZdc-Zd0qcH",
        "outputId": "387dcf82-9cc8-45e1-e265-8aceec385f51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n",
            "NVIDIA GeForce GTX 1650\n"
          ]
        }
      ],
      "source": [
        "#If GPU available then (TRUE) if not then (FALSE)\n",
        "print(torch.cuda.is_available())\n",
        "\n",
        "#INFO of GPU\n",
        "print(torch.cuda.get_device_name(0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "j_dF8Ytl0qcI"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
        "os.environ[\"TORCH_USE_CUDA_DSA\"] = \"1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kT0IH2Nw0qcI"
      },
      "source": [
        "Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Pl5kG-PJ0qcI"
      },
      "outputs": [],
      "source": [
        "#Hyperparameters\n",
        "num_episodes = 1000  \n",
        "epsilon_start = 1.0\n",
        "epsilon_end = 0.01\n",
        "epsilon_decay = 50000  \n",
        "gamma = 0.99\n",
        "batch_size = 32  \n",
        "lr = 1e-4\n",
        "memory_capacity = 5000  \n",
        "update_target_every = 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "6sHRCtbc0qcI"
      },
      "outputs": [],
      "source": [
        "def preprocess_frame(frame):\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
        "    resized = cv2.resize(gray, (84, 84), interpolation=cv2.INTER_AREA)\n",
        "    return resized / 255.0\n",
        "\n",
        "def stack_frames(stacked_frames, state, is_new_episode, stack_size=4):\n",
        "    frame = preprocess_frame(state)\n",
        "\n",
        "    if is_new_episode:\n",
        "        stacked_frames = deque([np.zeros((84, 84), dtype=np.float32) for _ in range(stack_size)], maxlen=stack_size)\n",
        "        for _ in range(stack_size):\n",
        "            stacked_frames.append(frame)\n",
        "    else:\n",
        "        stacked_frames.append(frame)\n",
        "\n",
        "    stacked_state = np.stack(stacked_frames, axis=0)  \n",
        "    return stacked_state, stacked_frames"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ulNREoT0qcI"
      },
      "source": [
        "ATARI Environment : DemonAttack-v5\n",
        "\n",
        "You are facing waves of demons in the ice planet of Krybor. Points are accumulated by destroying demons. You begin with 3 reserve bunkers, and can increase its number (up to 6) by avoiding enemy attacks. Each attack wave you survive without any hits, grants you a new bunker. Every time an enemy hits you, a bunker is destroyed. When the last bunker falls, the next enemy hit will destroy you and the game ends."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "env = gym.make(\"ALE/DemonAttack-v5\", render_mode='human')\n",
        "n_actions = env.action_space.n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "yx1Dw1Id0qcJ"
      },
      "outputs": [],
      "source": [
        "#DQN class\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, action_size):\n",
        "        super(DQN, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(4, 32, kernel_size=8, stride=4),  \n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(7 * 7 * 64, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, action_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(memory, policy_net, target_net, optimizer, batch_size, gamma, use_double_dqn, device):\n",
        "    if len(memory) < batch_size:\n",
        "        return\n",
        "\n",
        "    states, actions, rewards, next_states, dones = memory.sample(batch_size)\n",
        "\n",
        "    # Verify tensor shapes\n",
        "    assert states.shape[1:] == (4, 84, 84), f\"Expected shape [batch, 4, 84, 84], got {states.shape}\"\n",
        "    assert next_states.shape[1:] == (4, 84, 84), f\"Expected shape [batch, 4, 84, 84], got {next_states.shape}\"\n",
        "\n",
        "    states = torch.FloatTensor(states).to(device)\n",
        "    actions = torch.LongTensor(actions).unsqueeze(1).to(device)\n",
        "    rewards = torch.FloatTensor(rewards).unsqueeze(1).to(device)\n",
        "    next_states = torch.FloatTensor(next_states).to(device)\n",
        "    dones = torch.FloatTensor(dones).unsqueeze(1).to(device)\n",
        "\n",
        "    # Check for NaN or Inf values in next_states\n",
        "    if torch.isnan(next_states).any() or torch.isinf(next_states).any():\n",
        "        print(\"Warning: NaN or Inf detected in next_states tensor\")\n",
        "        return\n",
        "\n",
        "    q_values = policy_net(states).gather(1, actions)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        if use_double_dqn:\n",
        "            next_actions = policy_net(next_states).max(1)[1].unsqueeze(1)\n",
        "            next_q_values = target_net(next_states).gather(1, next_actions)\n",
        "        else:\n",
        "            next_q_values = target_net(next_states).max(1)[0].unsqueeze(1)\n",
        "\n",
        "        expected_q_values = rewards + (gamma * next_q_values * (1 - dones))\n",
        "\n",
        "    loss = F.mse_loss(q_values, expected_q_values)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(policy_net.parameters(), max_norm=1.0)\n",
        "    optimizer.step()\n",
        "    if device.type == \"cuda\":\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "    # Explicitly free tensors\n",
        "    del states, actions, rewards, next_states, dones, q_values, next_q_values, expected_q_values, loss\n",
        "    if device.type == \"cuda\":\n",
        "        torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "flJWY-ZI0qcJ"
      },
      "outputs": [],
      "source": [
        "#Replay Buffer\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        state, action, reward, next_state, done = map(np.stack, zip(*batch))\n",
        "        return state, action, reward, next_state, done\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "baCX0Xm30qcJ"
      },
      "outputs": [],
      "source": [
        "def select_action(state, steps_done, epsilon, policy_net, device, env):\n",
        "    if random.random() > epsilon:\n",
        "        with torch.no_grad():\n",
        "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)  \n",
        "            q_values = policy_net(state_tensor)\n",
        "            action = q_values.max(1)[1].item()\n",
        "            # Free memory\n",
        "            del state_tensor, q_values\n",
        "            if device.type == \"cuda\":\n",
        "                torch.cuda.empty_cache()\n",
        "            return action\n",
        "    else:\n",
        "        return env.action_space.sample()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "9Zu8k0Kd0qcK",
        "outputId": "3dffb47f-80a4-40ca-b0eb-a92f8eba6ed2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vanilla DQN | Episode 0, Reward: 205.0, Avg Epsilon: 0.9885\n",
            "Vanilla DQN | Episode 1, Reward: 80.0, Avg Epsilon: 0.9683\n",
            "Vanilla DQN | Episode 2, Reward: 150.0, Avg Epsilon: 0.9518\n",
            "Vanilla DQN | Episode 3, Reward: 110.0, Avg Epsilon: 0.9388\n",
            "Vanilla DQN | Episode 4, Reward: 160.0, Avg Epsilon: 0.9226\n",
            "Vanilla DQN | Episode 5, Reward: 130.0, Avg Epsilon: 0.9053\n",
            "Vanilla DQN | Episode 6, Reward: 235.0, Avg Epsilon: 0.8882\n",
            "Vanilla DQN | Episode 7, Reward: 90.0, Avg Epsilon: 0.8706\n",
            "Vanilla DQN | Episode 8, Reward: 90.0, Avg Epsilon: 0.8593\n",
            "Vanilla DQN | Episode 9, Reward: 130.0, Avg Epsilon: 0.8489\n",
            "Vanilla DQN | Episode 10, Reward: 205.0, Avg Epsilon: 0.8293\n",
            "Vanilla DQN | Episode 11, Reward: 265.0, Avg Epsilon: 0.8047\n",
            "Vanilla DQN | Episode 12, Reward: 120.0, Avg Epsilon: 0.7872\n",
            "Vanilla DQN | Episode 13, Reward: 80.0, Avg Epsilon: 0.7757\n",
            "Vanilla DQN | Episode 14, Reward: 80.0, Avg Epsilon: 0.7667\n",
            "Vanilla DQN | Episode 15, Reward: 220.0, Avg Epsilon: 0.7557\n",
            "Vanilla DQN | Episode 16, Reward: 190.0, Avg Epsilon: 0.7432\n",
            "Vanilla DQN | Episode 17, Reward: 80.0, Avg Epsilon: 0.7332\n",
            "Vanilla DQN | Episode 18, Reward: 235.0, Avg Epsilon: 0.7220\n",
            "Vanilla DQN | Episode 19, Reward: 175.0, Avg Epsilon: 0.7081\n",
            "Vanilla DQN | Episode 20, Reward: 150.0, Avg Epsilon: 0.6963\n",
            "Vanilla DQN | Episode 21, Reward: 110.0, Avg Epsilon: 0.6878\n",
            "Vanilla DQN | Episode 22, Reward: 70.0, Avg Epsilon: 0.6819\n",
            "Vanilla DQN | Episode 23, Reward: 150.0, Avg Epsilon: 0.6736\n",
            "Vanilla DQN | Episode 24, Reward: 40.0, Avg Epsilon: 0.6652\n",
            "Vanilla DQN | Episode 25, Reward: 140.0, Avg Epsilon: 0.6580\n",
            "Vanilla DQN | Episode 26, Reward: 160.0, Avg Epsilon: 0.6483\n",
            "Vanilla DQN | Episode 27, Reward: 280.0, Avg Epsilon: 0.6374\n",
            "Vanilla DQN | Episode 28, Reward: 90.0, Avg Epsilon: 0.6283\n",
            "Vanilla DQN | Episode 29, Reward: 295.0, Avg Epsilon: 0.6164\n",
            "Vanilla DQN | Episode 30, Reward: 80.0, Avg Epsilon: 0.6035\n",
            "Vanilla DQN | Episode 31, Reward: 70.0, Avg Epsilon: 0.5964\n",
            "Vanilla DQN | Episode 32, Reward: 140.0, Avg Epsilon: 0.5899\n",
            "Vanilla DQN | Episode 33, Reward: 50.0, Avg Epsilon: 0.5849\n",
            "Vanilla DQN | Episode 34, Reward: 100.0, Avg Epsilon: 0.5805\n",
            "Vanilla DQN | Episode 35, Reward: 235.0, Avg Epsilon: 0.5715\n",
            "Vanilla DQN | Episode 36, Reward: 90.0, Avg Epsilon: 0.5615\n",
            "Vanilla DQN | Episode 37, Reward: 90.0, Avg Epsilon: 0.5532\n",
            "Vanilla DQN | Episode 38, Reward: 190.0, Avg Epsilon: 0.5408\n",
            "Vanilla DQN | Episode 39, Reward: 90.0, Avg Epsilon: 0.5303\n",
            "Vanilla DQN | Episode 40, Reward: 100.0, Avg Epsilon: 0.5250\n",
            "Vanilla DQN | Episode 41, Reward: 140.0, Avg Epsilon: 0.5183\n",
            "Vanilla DQN | Episode 42, Reward: 50.0, Avg Epsilon: 0.5127\n",
            "Vanilla DQN | Episode 43, Reward: 50.0, Avg Epsilon: 0.5082\n",
            "Vanilla DQN | Episode 44, Reward: 130.0, Avg Epsilon: 0.5028\n",
            "Vanilla DQN | Episode 45, Reward: 60.0, Avg Epsilon: 0.4981\n",
            "Vanilla DQN | Episode 46, Reward: 100.0, Avg Epsilon: 0.4929\n",
            "Vanilla DQN | Episode 47, Reward: 70.0, Avg Epsilon: 0.4875\n",
            "Vanilla DQN | Episode 48, Reward: 175.0, Avg Epsilon: 0.4811\n",
            "Vanilla DQN | Episode 49, Reward: 100.0, Avg Epsilon: 0.4735\n",
            "Vanilla DQN | Episode 50, Reward: 130.0, Avg Epsilon: 0.4654\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\aayus\\AppData\\Local\\Temp\\ipykernel_22668\\2167289247.py:50: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  policy_net.load_state_dict(torch.load(save_path))\n",
            "C:\\Users\\aayus\\AppData\\Local\\Temp\\ipykernel_22668\\2167289247.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  target_net.load_state_dict(torch.load(save_path))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vanilla DQN | Episode 50, Loaded best model from vanilla_best.pt\n",
            "Vanilla DQN | Episode 51, Reward: 340.0, Avg Epsilon: 0.4539\n",
            "Vanilla DQN | Episode 52, Reward: 110.0, Avg Epsilon: 0.4443\n",
            "Vanilla DQN | Episode 53, Reward: 110.0, Avg Epsilon: 0.4385\n",
            "Vanilla DQN | Episode 54, Reward: 100.0, Avg Epsilon: 0.4336\n",
            "Vanilla DQN | Episode 55, Reward: 100.0, Avg Epsilon: 0.4283\n",
            "Vanilla DQN | Episode 56, Reward: 100.0, Avg Epsilon: 0.4228\n",
            "Vanilla DQN | Episode 57, Reward: 150.0, Avg Epsilon: 0.4175\n",
            "Vanilla DQN | Episode 58, Reward: 150.0, Avg Epsilon: 0.4121\n",
            "Vanilla DQN | Episode 59, Reward: 130.0, Avg Epsilon: 0.4067\n",
            "Vanilla DQN | Episode 60, Reward: 30.0, Avg Epsilon: 0.4027\n",
            "Vanilla DQN | Episode 61, Reward: 100.0, Avg Epsilon: 0.3993\n",
            "Vanilla DQN | Episode 62, Reward: 205.0, Avg Epsilon: 0.3930\n",
            "Vanilla DQN | Episode 63, Reward: 175.0, Avg Epsilon: 0.3868\n",
            "Vanilla DQN | Episode 64, Reward: 90.0, Avg Epsilon: 0.3825\n",
            "Vanilla DQN | Episode 65, Reward: 205.0, Avg Epsilon: 0.3772\n",
            "Vanilla DQN | Episode 66, Reward: 80.0, Avg Epsilon: 0.3718\n",
            "Vanilla DQN | Episode 67, Reward: 40.0, Avg Epsilon: 0.3690\n",
            "Vanilla DQN | Episode 68, Reward: 70.0, Avg Epsilon: 0.3671\n",
            "Vanilla DQN | Episode 69, Reward: 265.0, Avg Epsilon: 0.3630\n",
            "Vanilla DQN | Episode 70, Reward: 205.0, Avg Epsilon: 0.3563\n",
            "Vanilla DQN | Episode 71, Reward: 140.0, Avg Epsilon: 0.3499\n",
            "Vanilla DQN | Episode 72, Reward: 150.0, Avg Epsilon: 0.3451\n",
            "Vanilla DQN | Episode 73, Reward: 310.0, Avg Epsilon: 0.3386\n",
            "Vanilla DQN | Episode 74, Reward: 40.0, Avg Epsilon: 0.3335\n",
            "Vanilla DQN | Episode 75, Reward: 100.0, Avg Epsilon: 0.3307\n",
            "Vanilla DQN | Episode 76, Reward: 175.0, Avg Epsilon: 0.3259\n",
            "Vanilla DQN | Episode 77, Reward: 60.0, Avg Epsilon: 0.3221\n",
            "Vanilla DQN | Episode 78, Reward: 100.0, Avg Epsilon: 0.3192\n",
            "Vanilla DQN | Episode 79, Reward: 130.0, Avg Epsilon: 0.3142\n",
            "Vanilla DQN | Episode 80, Reward: 50.0, Avg Epsilon: 0.3103\n",
            "Vanilla DQN | Episode 81, Reward: 110.0, Avg Epsilon: 0.3079\n",
            "Vanilla DQN | Episode 82, Reward: 70.0, Avg Epsilon: 0.3053\n",
            "Vanilla DQN | Episode 83, Reward: 110.0, Avg Epsilon: 0.3025\n",
            "Vanilla DQN | Episode 84, Reward: 175.0, Avg Epsilon: 0.2980\n",
            "Vanilla DQN | Episode 85, Reward: 50.0, Avg Epsilon: 0.2944\n",
            "Vanilla DQN | Episode 86, Reward: 50.0, Avg Epsilon: 0.2927\n",
            "Vanilla DQN | Episode 87, Reward: 90.0, Avg Epsilon: 0.2907\n",
            "Vanilla DQN | Episode 88, Reward: 50.0, Avg Epsilon: 0.2887\n",
            "Vanilla DQN | Episode 89, Reward: 250.0, Avg Epsilon: 0.2843\n",
            "Vanilla DQN | Episode 90, Reward: 140.0, Avg Epsilon: 0.2794\n",
            "Vanilla DQN | Episode 91, Reward: 110.0, Avg Epsilon: 0.2757\n",
            "Vanilla DQN | Episode 92, Reward: 70.0, Avg Epsilon: 0.2718\n",
            "Vanilla DQN | Episode 93, Reward: 80.0, Avg Epsilon: 0.2687\n",
            "Vanilla DQN | Episode 94, Reward: 310.0, Avg Epsilon: 0.2635\n",
            "Vanilla DQN | Episode 95, Reward: 60.0, Avg Epsilon: 0.2590\n",
            "Vanilla DQN | Episode 96, Reward: 110.0, Avg Epsilon: 0.2565\n",
            "Vanilla DQN | Episode 97, Reward: 205.0, Avg Epsilon: 0.2531\n",
            "Vanilla DQN | Episode 98, Reward: 120.0, Avg Epsilon: 0.2501\n",
            "Vanilla DQN | Episode 99, Reward: 175.0, Avg Epsilon: 0.2470\n",
            "Vanilla DQN | Episode 100, Reward: 130.0, Avg Epsilon: 0.2430\n",
            "Vanilla DQN | Episode 100, Loaded best model from vanilla_best.pt\n",
            "Vanilla DQN | Episode 101, Reward: 220.0, Avg Epsilon: 0.2394\n",
            "Vanilla DQN | Episode 102, Reward: 100.0, Avg Epsilon: 0.2356\n",
            "Vanilla DQN | Episode 103, Reward: 60.0, Avg Epsilon: 0.2329\n",
            "Vanilla DQN | Episode 104, Reward: 140.0, Avg Epsilon: 0.2306\n",
            "Vanilla DQN | Episode 105, Reward: 80.0, Avg Epsilon: 0.2278\n",
            "Vanilla DQN | Episode 106, Reward: 130.0, Avg Epsilon: 0.2251\n",
            "Vanilla DQN | Episode 107, Reward: 120.0, Avg Epsilon: 0.2225\n",
            "Vanilla DQN | Episode 108, Reward: 90.0, Avg Epsilon: 0.2204\n",
            "Vanilla DQN | Episode 109, Reward: 100.0, Avg Epsilon: 0.2183\n",
            "Vanilla DQN | Episode 110, Reward: 110.0, Avg Epsilon: 0.2158\n",
            "Vanilla DQN | Episode 111, Reward: 175.0, Avg Epsilon: 0.2127\n",
            "Vanilla DQN | Episode 112, Reward: 80.0, Avg Epsilon: 0.2097\n",
            "Vanilla DQN | Episode 113, Reward: 120.0, Avg Epsilon: 0.2068\n",
            "Vanilla DQN | Episode 114, Reward: 100.0, Avg Epsilon: 0.2044\n",
            "Vanilla DQN | Episode 115, Reward: 110.0, Avg Epsilon: 0.2022\n",
            "Vanilla DQN | Episode 116, Reward: 60.0, Avg Epsilon: 0.2001\n",
            "Vanilla DQN | Episode 117, Reward: 220.0, Avg Epsilon: 0.1972\n",
            "Vanilla DQN | Episode 118, Reward: 175.0, Avg Epsilon: 0.1926\n",
            "Vanilla DQN | Episode 119, Reward: 60.0, Avg Epsilon: 0.1897\n",
            "Vanilla DQN | Episode 120, Reward: 130.0, Avg Epsilon: 0.1878\n",
            "Vanilla DQN | Episode 121, Reward: 150.0, Avg Epsilon: 0.1848\n",
            "Vanilla DQN | Episode 122, Reward: 90.0, Avg Epsilon: 0.1822\n",
            "Vanilla DQN | Episode 123, Reward: 130.0, Avg Epsilon: 0.1797\n",
            "Vanilla DQN | Episode 124, Reward: 90.0, Avg Epsilon: 0.1772\n",
            "Vanilla DQN | Episode 125, Reward: 220.0, Avg Epsilon: 0.1743\n",
            "Vanilla DQN | Episode 126, Reward: 100.0, Avg Epsilon: 0.1714\n",
            "Vanilla DQN | Episode 127, Reward: 120.0, Avg Epsilon: 0.1693\n",
            "Vanilla DQN | Episode 128, Reward: 60.0, Avg Epsilon: 0.1676\n",
            "Vanilla DQN | Episode 129, Reward: 80.0, Avg Epsilon: 0.1660\n",
            "Vanilla DQN | Episode 130, Reward: 130.0, Avg Epsilon: 0.1638\n",
            "Vanilla DQN | Episode 131, Reward: 150.0, Avg Epsilon: 0.1616\n",
            "Vanilla DQN | Episode 132, Reward: 120.0, Avg Epsilon: 0.1594\n",
            "Vanilla DQN | Episode 133, Reward: 265.0, Avg Epsilon: 0.1566\n",
            "Vanilla DQN | Episode 134, Reward: 160.0, Avg Epsilon: 0.1543\n",
            "Vanilla DQN | Episode 135, Reward: 325.0, Avg Epsilon: 0.1519\n",
            "Vanilla DQN | Episode 136, Reward: 140.0, Avg Epsilon: 0.1493\n",
            "Vanilla DQN | Episode 137, Reward: 265.0, Avg Epsilon: 0.1466\n",
            "Vanilla DQN | Episode 138, Reward: 60.0, Avg Epsilon: 0.1441\n",
            "Vanilla DQN | Episode 139, Reward: 130.0, Avg Epsilon: 0.1428\n",
            "Vanilla DQN | Episode 140, Reward: 70.0, Avg Epsilon: 0.1415\n",
            "Vanilla DQN | Episode 141, Reward: 60.0, Avg Epsilon: 0.1404\n",
            "Vanilla DQN | Episode 142, Reward: 175.0, Avg Epsilon: 0.1390\n",
            "Vanilla DQN | Episode 143, Reward: 70.0, Avg Epsilon: 0.1375\n",
            "Vanilla DQN | Episode 144, Reward: 130.0, Avg Epsilon: 0.1361\n",
            "Vanilla DQN | Episode 145, Reward: 40.0, Avg Epsilon: 0.1349\n",
            "Vanilla DQN | Episode 146, Reward: 220.0, Avg Epsilon: 0.1333\n",
            "Vanilla DQN | Episode 147, Reward: 175.0, Avg Epsilon: 0.1309\n",
            "Vanilla DQN | Episode 148, Reward: 190.0, Avg Epsilon: 0.1287\n",
            "Vanilla DQN | Episode 149, Reward: 140.0, Avg Epsilon: 0.1268\n",
            "Vanilla DQN | Episode 150, Reward: 40.0, Avg Epsilon: 0.1257\n",
            "Vanilla DQN | Episode 150, Loaded best model from vanilla_best.pt\n",
            "Vanilla DQN | Episode 151, Reward: 70.0, Avg Epsilon: 0.1248\n",
            "Vanilla DQN | Episode 152, Reward: 30.0, Avg Epsilon: 0.1240\n",
            "Vanilla DQN | Episode 153, Reward: 130.0, Avg Epsilon: 0.1230\n",
            "Vanilla DQN | Episode 154, Reward: 150.0, Avg Epsilon: 0.1219\n",
            "Vanilla DQN | Episode 155, Reward: 100.0, Avg Epsilon: 0.1206\n",
            "Vanilla DQN | Episode 156, Reward: 120.0, Avg Epsilon: 0.1193\n",
            "Vanilla DQN | Episode 157, Reward: 70.0, Avg Epsilon: 0.1180\n",
            "Vanilla DQN | Episode 158, Reward: 80.0, Avg Epsilon: 0.1168\n",
            "Vanilla DQN | Episode 159, Reward: 90.0, Avg Epsilon: 0.1158\n",
            "Vanilla DQN | Episode 160, Reward: 140.0, Avg Epsilon: 0.1145\n",
            "Vanilla DQN | Episode 161, Reward: 90.0, Avg Epsilon: 0.1132\n",
            "Vanilla DQN | Episode 162, Reward: 80.0, Avg Epsilon: 0.1121\n",
            "Vanilla DQN | Episode 163, Reward: 130.0, Avg Epsilon: 0.1110\n",
            "Vanilla DQN | Episode 164, Reward: 70.0, Avg Epsilon: 0.1097\n",
            "Vanilla DQN | Episode 165, Reward: 90.0, Avg Epsilon: 0.1083\n",
            "Vanilla DQN | Episode 166, Reward: 175.0, Avg Epsilon: 0.1067\n",
            "Vanilla DQN | Episode 167, Reward: 80.0, Avg Epsilon: 0.1054\n",
            "Vanilla DQN | Episode 168, Reward: 80.0, Avg Epsilon: 0.1042\n",
            "Vanilla DQN | Episode 169, Reward: 110.0, Avg Epsilon: 0.1029\n",
            "Vanilla DQN | Episode 170, Reward: 140.0, Avg Epsilon: 0.1016\n",
            "Vanilla DQN | Episode 171, Reward: 130.0, Avg Epsilon: 0.0986\n",
            "Vanilla DQN | Episode 172, Reward: 130.0, Avg Epsilon: 0.0957\n",
            "Vanilla DQN | Episode 173, Reward: 140.0, Avg Epsilon: 0.0924\n",
            "Vanilla DQN | Episode 174, Reward: 140.0, Avg Epsilon: 0.0888\n",
            "Vanilla DQN | Episode 175, Reward: 120.0, Avg Epsilon: 0.0872\n",
            "Vanilla DQN | Episode 176, Reward: 120.0, Avg Epsilon: 0.0863\n",
            "Vanilla DQN | Episode 177, Reward: 160.0, Avg Epsilon: 0.0853\n",
            "Vanilla DQN | Episode 178, Reward: 50.0, Avg Epsilon: 0.0845\n",
            "Vanilla DQN | Episode 179, Reward: 250.0, Avg Epsilon: 0.0836\n",
            "Vanilla DQN | Episode 180, Reward: 130.0, Avg Epsilon: 0.0825\n",
            "Vanilla DQN | Episode 181, Reward: 80.0, Avg Epsilon: 0.0817\n",
            "Vanilla DQN | Episode 182, Reward: 70.0, Avg Epsilon: 0.0811\n",
            "Vanilla DQN | Episode 183, Reward: 90.0, Avg Epsilon: 0.0805\n",
            "Vanilla DQN | Episode 184, Reward: 80.0, Avg Epsilon: 0.0799\n",
            "Vanilla DQN | Episode 185, Reward: 190.0, Avg Epsilon: 0.0787\n",
            "Vanilla DQN | Episode 186, Reward: 140.0, Avg Epsilon: 0.0775\n",
            "Vanilla DQN | Episode 187, Reward: 50.0, Avg Epsilon: 0.0770\n",
            "Vanilla DQN | Episode 188, Reward: 40.0, Avg Epsilon: 0.0765\n",
            "Vanilla DQN | Episode 189, Reward: 130.0, Avg Epsilon: 0.0758\n",
            "Vanilla DQN | Episode 190, Reward: 205.0, Avg Epsilon: 0.0747\n",
            "Vanilla DQN | Episode 191, Reward: 90.0, Avg Epsilon: 0.0739\n",
            "Vanilla DQN | Episode 192, Reward: 110.0, Avg Epsilon: 0.0732\n",
            "Vanilla DQN | Episode 193, Reward: 90.0, Avg Epsilon: 0.0725\n",
            "Vanilla DQN | Episode 194, Reward: 130.0, Avg Epsilon: 0.0719\n",
            "Vanilla DQN | Episode 195, Reward: 50.0, Avg Epsilon: 0.0713\n",
            "Vanilla DQN | Episode 196, Reward: 150.0, Avg Epsilon: 0.0708\n",
            "Vanilla DQN | Episode 197, Reward: 205.0, Avg Epsilon: 0.0698\n",
            "Vanilla DQN | Episode 198, Reward: 70.0, Avg Epsilon: 0.0691\n",
            "Vanilla DQN | Episode 199, Reward: 250.0, Avg Epsilon: 0.0683\n",
            "Vanilla DQN | Episode 200, Reward: 60.0, Avg Epsilon: 0.0675\n",
            "Vanilla DQN | Episode 200, Loaded best model from vanilla_best.pt\n",
            "Vanilla DQN | Episode 201, Reward: 120.0, Avg Epsilon: 0.0669\n",
            "Vanilla DQN | Episode 202, Reward: 80.0, Avg Epsilon: 0.0663\n",
            "Vanilla DQN | Episode 203, Reward: 295.0, Avg Epsilon: 0.0656\n",
            "Vanilla DQN | Episode 204, Reward: 220.0, Avg Epsilon: 0.0645\n",
            "Vanilla DQN | Episode 205, Reward: 110.0, Avg Epsilon: 0.0637\n",
            "Vanilla DQN | Episode 206, Reward: 120.0, Avg Epsilon: 0.0631\n",
            "Vanilla DQN | Episode 207, Reward: 250.0, Avg Epsilon: 0.0622\n",
            "Vanilla DQN | Episode 208, Reward: 150.0, Avg Epsilon: 0.0613\n",
            "Vanilla DQN | Episode 209, Reward: 140.0, Avg Epsilon: 0.0605\n",
            "Vanilla DQN | Episode 210, Reward: 90.0, Avg Epsilon: 0.0599\n",
            "Vanilla DQN | Episode 211, Reward: 100.0, Avg Epsilon: 0.0594\n",
            "Vanilla DQN | Episode 212, Reward: 80.0, Avg Epsilon: 0.0588\n",
            "Vanilla DQN | Episode 213, Reward: 100.0, Avg Epsilon: 0.0582\n",
            "Vanilla DQN | Episode 214, Reward: 150.0, Avg Epsilon: 0.0577\n",
            "Vanilla DQN | Episode 215, Reward: 80.0, Avg Epsilon: 0.0572\n",
            "Vanilla DQN | Episode 216, Reward: 70.0, Avg Epsilon: 0.0569\n",
            "Vanilla DQN | Episode 217, Reward: 100.0, Avg Epsilon: 0.0564\n",
            "Vanilla DQN | Episode 218, Reward: 80.0, Avg Epsilon: 0.0559\n",
            "Vanilla DQN | Episode 219, Reward: 90.0, Avg Epsilon: 0.0555\n",
            "Vanilla DQN | Episode 220, Reward: 90.0, Avg Epsilon: 0.0551\n",
            "Vanilla DQN | Episode 221, Reward: 100.0, Avg Epsilon: 0.0546\n",
            "Vanilla DQN | Episode 222, Reward: 220.0, Avg Epsilon: 0.0541\n",
            "Vanilla DQN | Episode 223, Reward: 70.0, Avg Epsilon: 0.0535\n",
            "Vanilla DQN | Episode 224, Reward: 80.0, Avg Epsilon: 0.0531\n",
            "Vanilla DQN | Episode 225, Reward: 90.0, Avg Epsilon: 0.0528\n",
            "Vanilla DQN | Episode 226, Reward: 150.0, Avg Epsilon: 0.0523\n",
            "Vanilla DQN | Episode 227, Reward: 100.0, Avg Epsilon: 0.0518\n",
            "Vanilla DQN | Episode 228, Reward: 120.0, Avg Epsilon: 0.0514\n",
            "Vanilla DQN | Episode 229, Reward: 110.0, Avg Epsilon: 0.0509\n",
            "Vanilla DQN | Episode 230, Reward: 100.0, Avg Epsilon: 0.0504\n",
            "Vanilla DQN | Episode 231, Reward: 130.0, Avg Epsilon: 0.0499\n",
            "Vanilla DQN | Episode 232, Reward: 60.0, Avg Epsilon: 0.0495\n",
            "Vanilla DQN | Episode 233, Reward: 130.0, Avg Epsilon: 0.0492\n",
            "Vanilla DQN | Episode 234, Reward: 130.0, Avg Epsilon: 0.0488\n",
            "Vanilla DQN | Episode 235, Reward: 280.0, Avg Epsilon: 0.0482\n",
            "Vanilla DQN | Episode 236, Reward: 205.0, Avg Epsilon: 0.0475\n",
            "Vanilla DQN | Episode 237, Reward: 120.0, Avg Epsilon: 0.0469\n",
            "Vanilla DQN | Episode 238, Reward: 70.0, Avg Epsilon: 0.0466\n",
            "Vanilla DQN | Episode 239, Reward: 130.0, Avg Epsilon: 0.0463\n",
            "Vanilla DQN | Episode 240, Reward: 150.0, Avg Epsilon: 0.0455\n",
            "Vanilla DQN | Episode 241, Reward: 80.0, Avg Epsilon: 0.0447\n",
            "Vanilla DQN | Episode 242, Reward: 120.0, Avg Epsilon: 0.0443\n",
            "Vanilla DQN | Episode 243, Reward: 110.0, Avg Epsilon: 0.0438\n",
            "Vanilla DQN | Episode 244, Reward: 60.0, Avg Epsilon: 0.0436\n",
            "Vanilla DQN | Episode 245, Reward: 150.0, Avg Epsilon: 0.0433\n",
            "Vanilla DQN | Episode 246, Reward: 235.0, Avg Epsilon: 0.0429\n",
            "Vanilla DQN | Episode 247, Reward: 70.0, Avg Epsilon: 0.0425\n",
            "Vanilla DQN | Episode 248, Reward: 90.0, Avg Epsilon: 0.0421\n",
            "Vanilla DQN | Episode 249, Reward: 150.0, Avg Epsilon: 0.0416\n",
            "Vanilla DQN | Episode 250, Reward: 370.0, Avg Epsilon: 0.0408\n",
            "Vanilla DQN | Episode 250, Loaded best model from vanilla_best.pt\n",
            "Vanilla DQN | Episode 251, Reward: 40.0, Avg Epsilon: 0.0402\n",
            "Vanilla DQN | Episode 252, Reward: 190.0, Avg Epsilon: 0.0390\n",
            "Vanilla DQN | Episode 253, Reward: 60.0, Avg Epsilon: 0.0379\n",
            "Vanilla DQN | Episode 254, Reward: 90.0, Avg Epsilon: 0.0377\n",
            "Vanilla DQN | Episode 255, Reward: 220.0, Avg Epsilon: 0.0374\n",
            "Vanilla DQN | Episode 256, Reward: 235.0, Avg Epsilon: 0.0369\n",
            "Vanilla DQN | Episode 257, Reward: 160.0, Avg Epsilon: 0.0364\n",
            "Vanilla DQN | Episode 258, Reward: 265.0, Avg Epsilon: 0.0358\n",
            "Vanilla DQN | Episode 259, Reward: 150.0, Avg Epsilon: 0.0353\n",
            "Vanilla DQN | Episode 260, Reward: 110.0, Avg Epsilon: 0.0349\n",
            "Vanilla DQN | Episode 261, Reward: 70.0, Avg Epsilon: 0.0347\n",
            "Vanilla DQN | Episode 262, Reward: 80.0, Avg Epsilon: 0.0344\n",
            "Vanilla DQN | Episode 263, Reward: 110.0, Avg Epsilon: 0.0342\n",
            "Vanilla DQN | Episode 264, Reward: 150.0, Avg Epsilon: 0.0339\n",
            "Vanilla DQN | Episode 265, Reward: 295.0, Avg Epsilon: 0.0327\n",
            "Vanilla DQN | Episode 266, Reward: 40.0, Avg Epsilon: 0.0316\n",
            "Vanilla DQN | Episode 267, Reward: 160.0, Avg Epsilon: 0.0313\n",
            "Vanilla DQN | Episode 268, Reward: 175.0, Avg Epsilon: 0.0310\n",
            "Vanilla DQN | Episode 269, Reward: 120.0, Avg Epsilon: 0.0305\n",
            "Vanilla DQN | Episode 270, Reward: 90.0, Avg Epsilon: 0.0302\n",
            "Vanilla DQN | Episode 271, Reward: 90.0, Avg Epsilon: 0.0300\n",
            "Vanilla DQN | Episode 272, Reward: 100.0, Avg Epsilon: 0.0298\n",
            "Vanilla DQN | Episode 273, Reward: 140.0, Avg Epsilon: 0.0295\n",
            "Vanilla DQN | Episode 274, Reward: 120.0, Avg Epsilon: 0.0292\n",
            "Vanilla DQN | Episode 275, Reward: 60.0, Avg Epsilon: 0.0290\n",
            "Vanilla DQN | Episode 276, Reward: 130.0, Avg Epsilon: 0.0288\n",
            "Vanilla DQN | Episode 277, Reward: 160.0, Avg Epsilon: 0.0285\n",
            "Vanilla DQN | Episode 278, Reward: 0.0, Avg Epsilon: 0.0283\n",
            "Vanilla DQN | Episode 279, Reward: 80.0, Avg Epsilon: 0.0280\n",
            "Vanilla DQN | Episode 280, Reward: 40.0, Avg Epsilon: 0.0277\n",
            "Vanilla DQN | Episode 281, Reward: 160.0, Avg Epsilon: 0.0274\n",
            "Vanilla DQN | Episode 282, Reward: 100.0, Avg Epsilon: 0.0270\n",
            "Vanilla DQN | Episode 283, Reward: 70.0, Avg Epsilon: 0.0268\n",
            "Vanilla DQN | Episode 284, Reward: 110.0, Avg Epsilon: 0.0266\n",
            "Vanilla DQN | Episode 285, Reward: 140.0, Avg Epsilon: 0.0263\n",
            "Vanilla DQN | Episode 286, Reward: 175.0, Avg Epsilon: 0.0260\n",
            "Vanilla DQN | Episode 287, Reward: 265.0, Avg Epsilon: 0.0257\n",
            "Vanilla DQN | Episode 288, Reward: 355.0, Avg Epsilon: 0.0253\n",
            "Vanilla DQN | Episode 289, Reward: 110.0, Avg Epsilon: 0.0249\n",
            "Vanilla DQN | Episode 290, Reward: 100.0, Avg Epsilon: 0.0247\n",
            "Vanilla DQN | Episode 291, Reward: 90.0, Avg Epsilon: 0.0245\n",
            "Vanilla DQN | Episode 292, Reward: 160.0, Avg Epsilon: 0.0243\n",
            "Vanilla DQN | Episode 293, Reward: 175.0, Avg Epsilon: 0.0240\n",
            "Vanilla DQN | Episode 294, Reward: 90.0, Avg Epsilon: 0.0238\n",
            "Vanilla DQN | Episode 295, Reward: 150.0, Avg Epsilon: 0.0236\n",
            "Vanilla DQN | Episode 296, Reward: 70.0, Avg Epsilon: 0.0234\n",
            "Vanilla DQN | Episode 297, Reward: 150.0, Avg Epsilon: 0.0233\n",
            "Vanilla DQN | Episode 298, Reward: 175.0, Avg Epsilon: 0.0231\n",
            "Vanilla DQN | Episode 299, Reward: 130.0, Avg Epsilon: 0.0229\n",
            "Vanilla DQN | Episode 300, Reward: 90.0, Avg Epsilon: 0.0228\n",
            "Vanilla DQN | Episode 300, Loaded best model from vanilla_best.pt\n",
            "Vanilla DQN | Episode 301, Reward: 150.0, Avg Epsilon: 0.0227\n",
            "Vanilla DQN | Episode 302, Reward: 130.0, Avg Epsilon: 0.0225\n",
            "Vanilla DQN | Episode 303, Reward: 100.0, Avg Epsilon: 0.0223\n",
            "Vanilla DQN | Episode 304, Reward: 130.0, Avg Epsilon: 0.0221\n",
            "Vanilla DQN | Episode 305, Reward: 100.0, Avg Epsilon: 0.0220\n",
            "Vanilla DQN | Episode 306, Reward: 50.0, Avg Epsilon: 0.0219\n",
            "Vanilla DQN | Episode 307, Reward: 70.0, Avg Epsilon: 0.0217\n",
            "Vanilla DQN | Episode 308, Reward: 130.0, Avg Epsilon: 0.0216\n",
            "Vanilla DQN | Episode 309, Reward: 90.0, Avg Epsilon: 0.0215\n",
            "Vanilla DQN | Episode 310, Reward: 150.0, Avg Epsilon: 0.0214\n",
            "Vanilla DQN | Episode 311, Reward: 90.0, Avg Epsilon: 0.0212\n",
            "Vanilla DQN | Episode 312, Reward: 220.0, Avg Epsilon: 0.0210\n",
            "Vanilla DQN | Episode 313, Reward: 280.0, Avg Epsilon: 0.0208\n",
            "Vanilla DQN | Episode 314, Reward: 190.0, Avg Epsilon: 0.0206\n",
            "Vanilla DQN | Episode 315, Reward: 140.0, Avg Epsilon: 0.0204\n",
            "Vanilla DQN | Episode 316, Reward: 110.0, Avg Epsilon: 0.0202\n",
            "Vanilla DQN | Episode 317, Reward: 205.0, Avg Epsilon: 0.0201\n",
            "Vanilla DQN | Episode 318, Reward: 90.0, Avg Epsilon: 0.0199\n",
            "Vanilla DQN | Episode 319, Reward: 120.0, Avg Epsilon: 0.0198\n",
            "Vanilla DQN | Episode 320, Reward: 130.0, Avg Epsilon: 0.0197\n",
            "Vanilla DQN | Episode 321, Reward: 90.0, Avg Epsilon: 0.0196\n",
            "Vanilla DQN | Episode 322, Reward: 70.0, Avg Epsilon: 0.0195\n",
            "Vanilla DQN | Episode 323, Reward: 80.0, Avg Epsilon: 0.0194\n",
            "Vanilla DQN | Episode 324, Reward: 190.0, Avg Epsilon: 0.0193\n",
            "Vanilla DQN | Episode 325, Reward: 190.0, Avg Epsilon: 0.0191\n",
            "Vanilla DQN | Episode 326, Reward: 70.0, Avg Epsilon: 0.0190\n",
            "Vanilla DQN | Episode 327, Reward: 150.0, Avg Epsilon: 0.0189\n",
            "Vanilla DQN | Episode 328, Reward: 220.0, Avg Epsilon: 0.0188\n",
            "Vanilla DQN | Episode 329, Reward: 90.0, Avg Epsilon: 0.0187\n",
            "Vanilla DQN | Episode 330, Reward: 80.0, Avg Epsilon: 0.0186\n",
            "Vanilla DQN | Episode 331, Reward: 120.0, Avg Epsilon: 0.0185\n",
            "Vanilla DQN | Episode 332, Reward: 220.0, Avg Epsilon: 0.0184\n",
            "Vanilla DQN | Episode 333, Reward: 130.0, Avg Epsilon: 0.0183\n",
            "Vanilla DQN | Episode 334, Reward: 130.0, Avg Epsilon: 0.0182\n",
            "Vanilla DQN | Episode 335, Reward: 90.0, Avg Epsilon: 0.0181\n",
            "Vanilla DQN | Episode 336, Reward: 90.0, Avg Epsilon: 0.0180\n",
            "Vanilla DQN | Episode 337, Reward: 110.0, Avg Epsilon: 0.0179\n",
            "Vanilla DQN | Episode 338, Reward: 100.0, Avg Epsilon: 0.0178\n",
            "Vanilla DQN | Episode 339, Reward: 80.0, Avg Epsilon: 0.0178\n",
            "Vanilla DQN | Episode 340, Reward: 130.0, Avg Epsilon: 0.0176\n",
            "Vanilla DQN | Episode 341, Reward: 100.0, Avg Epsilon: 0.0175\n",
            "Vanilla DQN | Episode 342, Reward: 140.0, Avg Epsilon: 0.0174\n",
            "Vanilla DQN | Episode 343, Reward: 90.0, Avg Epsilon: 0.0173\n",
            "Vanilla DQN | Episode 344, Reward: 70.0, Avg Epsilon: 0.0173\n",
            "Vanilla DQN | Episode 345, Reward: 110.0, Avg Epsilon: 0.0172\n",
            "Vanilla DQN | Episode 346, Reward: 175.0, Avg Epsilon: 0.0171\n",
            "Vanilla DQN | Episode 347, Reward: 160.0, Avg Epsilon: 0.0170\n",
            "Vanilla DQN | Episode 348, Reward: 90.0, Avg Epsilon: 0.0169\n",
            "Vanilla DQN | Episode 349, Reward: 90.0, Avg Epsilon: 0.0168\n",
            "Vanilla DQN | Episode 350, Reward: 140.0, Avg Epsilon: 0.0167\n",
            "Vanilla DQN | Episode 350, Loaded best model from vanilla_best.pt\n",
            "Vanilla DQN | Episode 351, Reward: 100.0, Avg Epsilon: 0.0166\n",
            "Vanilla DQN | Episode 352, Reward: 175.0, Avg Epsilon: 0.0165\n",
            "Vanilla DQN | Episode 353, Reward: 100.0, Avg Epsilon: 0.0164\n",
            "Vanilla DQN | Episode 354, Reward: 50.0, Avg Epsilon: 0.0163\n",
            "Vanilla DQN | Episode 355, Reward: 130.0, Avg Epsilon: 0.0163\n",
            "Vanilla DQN | Episode 356, Reward: 140.0, Avg Epsilon: 0.0161\n",
            "Vanilla DQN | Episode 357, Reward: 80.0, Avg Epsilon: 0.0160\n",
            "Vanilla DQN | Episode 358, Reward: 60.0, Avg Epsilon: 0.0160\n",
            "Vanilla DQN | Episode 359, Reward: 205.0, Avg Epsilon: 0.0159\n",
            "Vanilla DQN | Episode 360, Reward: 120.0, Avg Epsilon: 0.0158\n",
            "Vanilla DQN | Episode 361, Reward: 40.0, Avg Epsilon: 0.0157\n",
            "Vanilla DQN | Episode 362, Reward: 100.0, Avg Epsilon: 0.0157\n",
            "Vanilla DQN | Episode 363, Reward: 140.0, Avg Epsilon: 0.0156\n",
            "Vanilla DQN | Episode 364, Reward: 130.0, Avg Epsilon: 0.0155\n",
            "Vanilla DQN | Episode 365, Reward: 190.0, Avg Epsilon: 0.0155\n",
            "Vanilla DQN | Episode 366, Reward: 110.0, Avg Epsilon: 0.0154\n",
            "Vanilla DQN | Episode 367, Reward: 140.0, Avg Epsilon: 0.0153\n",
            "Vanilla DQN | Episode 368, Reward: 60.0, Avg Epsilon: 0.0153\n",
            "Vanilla DQN | Episode 369, Reward: 295.0, Avg Epsilon: 0.0152\n",
            "Vanilla DQN | Episode 370, Reward: 235.0, Avg Epsilon: 0.0151\n",
            "Vanilla DQN | Episode 371, Reward: 30.0, Avg Epsilon: 0.0150\n",
            "Vanilla DQN | Episode 372, Reward: 140.0, Avg Epsilon: 0.0150\n",
            "Vanilla DQN | Episode 373, Reward: 190.0, Avg Epsilon: 0.0149\n",
            "Vanilla DQN | Episode 374, Reward: 220.0, Avg Epsilon: 0.0147\n",
            "Vanilla DQN | Episode 375, Reward: 70.0, Avg Epsilon: 0.0145\n",
            "Vanilla DQN | Episode 376, Reward: 30.0, Avg Epsilon: 0.0144\n",
            "Vanilla DQN | Episode 377, Reward: 235.0, Avg Epsilon: 0.0144\n",
            "Vanilla DQN | Episode 378, Reward: 100.0, Avg Epsilon: 0.0143\n",
            "Vanilla DQN | Episode 379, Reward: 220.0, Avg Epsilon: 0.0142\n",
            "Vanilla DQN | Episode 380, Reward: 50.0, Avg Epsilon: 0.0141\n",
            "Vanilla DQN | Episode 381, Reward: 220.0, Avg Epsilon: 0.0141\n",
            "Vanilla DQN | Episode 382, Reward: 60.0, Avg Epsilon: 0.0140\n",
            "Vanilla DQN | Episode 383, Reward: 60.0, Avg Epsilon: 0.0140\n",
            "Vanilla DQN | Episode 384, Reward: 130.0, Avg Epsilon: 0.0140\n",
            "Vanilla DQN | Episode 385, Reward: 100.0, Avg Epsilon: 0.0139\n",
            "Vanilla DQN | Episode 386, Reward: 130.0, Avg Epsilon: 0.0139\n",
            "Vanilla DQN | Episode 387, Reward: 80.0, Avg Epsilon: 0.0138\n",
            "Vanilla DQN | Episode 388, Reward: 150.0, Avg Epsilon: 0.0138\n",
            "Vanilla DQN | Episode 389, Reward: 160.0, Avg Epsilon: 0.0137\n",
            "Vanilla DQN | Episode 390, Reward: 160.0, Avg Epsilon: 0.0136\n",
            "Vanilla DQN | Episode 391, Reward: 235.0, Avg Epsilon: 0.0135\n",
            "Vanilla DQN | Episode 392, Reward: 175.0, Avg Epsilon: 0.0135\n",
            "Vanilla DQN | Episode 393, Reward: 130.0, Avg Epsilon: 0.0134\n",
            "Vanilla DQN | Episode 394, Reward: 175.0, Avg Epsilon: 0.0134\n",
            "Vanilla DQN | Episode 395, Reward: 80.0, Avg Epsilon: 0.0133\n",
            "Vanilla DQN | Episode 396, Reward: 90.0, Avg Epsilon: 0.0133\n",
            "Vanilla DQN | Episode 397, Reward: 130.0, Avg Epsilon: 0.0132\n",
            "Vanilla DQN | Episode 398, Reward: 130.0, Avg Epsilon: 0.0132\n",
            "Vanilla DQN | Episode 399, Reward: 140.0, Avg Epsilon: 0.0131\n",
            "Vanilla DQN | Episode 400, Reward: 60.0, Avg Epsilon: 0.0131\n",
            "Vanilla DQN | Episode 400, Loaded best model from vanilla_best.pt\n",
            "Vanilla DQN | Episode 401, Reward: 80.0, Avg Epsilon: 0.0131\n",
            "Vanilla DQN | Episode 402, Reward: 60.0, Avg Epsilon: 0.0131\n",
            "Vanilla DQN | Episode 403, Reward: 250.0, Avg Epsilon: 0.0130\n",
            "Vanilla DQN | Episode 404, Reward: 250.0, Avg Epsilon: 0.0129\n",
            "Vanilla DQN | Episode 405, Reward: 160.0, Avg Epsilon: 0.0128\n",
            "Vanilla DQN | Episode 406, Reward: 90.0, Avg Epsilon: 0.0128\n",
            "Vanilla DQN | Episode 407, Reward: 50.0, Avg Epsilon: 0.0128\n",
            "Vanilla DQN | Episode 408, Reward: 235.0, Avg Epsilon: 0.0127\n",
            "Vanilla DQN | Episode 409, Reward: 110.0, Avg Epsilon: 0.0127\n",
            "Vanilla DQN | Episode 410, Reward: 70.0, Avg Epsilon: 0.0127\n",
            "Vanilla DQN | Episode 411, Reward: 150.0, Avg Epsilon: 0.0126\n",
            "Vanilla DQN | Episode 412, Reward: 175.0, Avg Epsilon: 0.0126\n",
            "Vanilla DQN | Episode 413, Reward: 70.0, Avg Epsilon: 0.0126\n",
            "Vanilla DQN | Episode 414, Reward: 110.0, Avg Epsilon: 0.0125\n",
            "Vanilla DQN | Episode 415, Reward: 265.0, Avg Epsilon: 0.0125\n",
            "Vanilla DQN | Episode 416, Reward: 295.0, Avg Epsilon: 0.0124\n",
            "Vanilla DQN | Episode 417, Reward: 310.0, Avg Epsilon: 0.0123\n",
            "Vanilla DQN | Episode 418, Reward: 100.0, Avg Epsilon: 0.0122\n",
            "Vanilla DQN | Episode 419, Reward: 175.0, Avg Epsilon: 0.0122\n",
            "Vanilla DQN | Episode 420, Reward: 120.0, Avg Epsilon: 0.0122\n",
            "Vanilla DQN | Episode 421, Reward: 120.0, Avg Epsilon: 0.0122\n",
            "Vanilla DQN | Episode 422, Reward: 265.0, Avg Epsilon: 0.0121\n",
            "Vanilla DQN | Episode 423, Reward: 40.0, Avg Epsilon: 0.0121\n",
            "Vanilla DQN | Episode 424, Reward: 130.0, Avg Epsilon: 0.0121\n",
            "Vanilla DQN | Episode 425, Reward: 130.0, Avg Epsilon: 0.0120\n",
            "Vanilla DQN | Episode 426, Reward: 120.0, Avg Epsilon: 0.0120\n",
            "Vanilla DQN | Episode 427, Reward: 70.0, Avg Epsilon: 0.0120\n",
            "Vanilla DQN | Episode 428, Reward: 80.0, Avg Epsilon: 0.0119\n",
            "Vanilla DQN | Episode 429, Reward: 120.0, Avg Epsilon: 0.0119\n",
            "Vanilla DQN | Episode 430, Reward: 120.0, Avg Epsilon: 0.0119\n",
            "Vanilla DQN | Episode 431, Reward: 40.0, Avg Epsilon: 0.0118\n",
            "Vanilla DQN | Episode 432, Reward: 355.0, Avg Epsilon: 0.0118\n",
            "Vanilla DQN | Episode 433, Reward: 130.0, Avg Epsilon: 0.0117\n",
            "Vanilla DQN | Episode 434, Reward: 175.0, Avg Epsilon: 0.0117\n",
            "Vanilla DQN | Episode 435, Reward: 160.0, Avg Epsilon: 0.0117\n",
            "Vanilla DQN | Episode 436, Reward: 130.0, Avg Epsilon: 0.0116\n",
            "Vanilla DQN | Episode 437, Reward: 110.0, Avg Epsilon: 0.0116\n",
            "Vanilla DQN | Episode 438, Reward: 250.0, Avg Epsilon: 0.0116\n",
            "Vanilla DQN | Episode 439, Reward: 70.0, Avg Epsilon: 0.0115\n",
            "Vanilla DQN | Episode 440, Reward: 60.0, Avg Epsilon: 0.0115\n",
            "Vanilla DQN | Episode 441, Reward: 90.0, Avg Epsilon: 0.0115\n",
            "Vanilla DQN | Episode 442, Reward: 160.0, Avg Epsilon: 0.0115\n",
            "Vanilla DQN | Episode 443, Reward: 100.0, Avg Epsilon: 0.0115\n",
            "Vanilla DQN | Episode 444, Reward: 80.0, Avg Epsilon: 0.0115\n",
            "Vanilla DQN | Episode 445, Reward: 150.0, Avg Epsilon: 0.0114\n",
            "Vanilla DQN | Episode 446, Reward: 160.0, Avg Epsilon: 0.0114\n",
            "Vanilla DQN | Episode 447, Reward: 160.0, Avg Epsilon: 0.0114\n",
            "Vanilla DQN | Episode 448, Reward: 205.0, Avg Epsilon: 0.0113\n",
            "Vanilla DQN | Episode 449, Reward: 90.0, Avg Epsilon: 0.0113\n",
            "Vanilla DQN | Episode 450, Reward: 100.0, Avg Epsilon: 0.0113\n",
            "Vanilla DQN | Episode 450, Loaded best model from vanilla_best.pt\n",
            "Vanilla DQN | Episode 451, Reward: 120.0, Avg Epsilon: 0.0113\n",
            "Vanilla DQN | Episode 452, Reward: 70.0, Avg Epsilon: 0.0113\n",
            "Vanilla DQN | Episode 453, Reward: 250.0, Avg Epsilon: 0.0112\n",
            "Vanilla DQN | Episode 454, Reward: 120.0, Avg Epsilon: 0.0112\n",
            "Vanilla DQN | Episode 455, Reward: 175.0, Avg Epsilon: 0.0112\n",
            "Vanilla DQN | Episode 456, Reward: 150.0, Avg Epsilon: 0.0112\n",
            "Vanilla DQN | Episode 457, Reward: 110.0, Avg Epsilon: 0.0112\n",
            "Vanilla DQN | Episode 458, Reward: 205.0, Avg Epsilon: 0.0111\n",
            "Vanilla DQN | Episode 459, Reward: 120.0, Avg Epsilon: 0.0111\n",
            "Vanilla DQN | Episode 460, Reward: 120.0, Avg Epsilon: 0.0111\n",
            "Vanilla DQN | Episode 461, Reward: 310.0, Avg Epsilon: 0.0111\n",
            "Vanilla DQN | Episode 462, Reward: 370.0, Avg Epsilon: 0.0110\n",
            "Vanilla DQN | Episode 463, Reward: 235.0, Avg Epsilon: 0.0110\n",
            "Vanilla DQN | Episode 464, Reward: 110.0, Avg Epsilon: 0.0110\n",
            "Vanilla DQN | Episode 465, Reward: 190.0, Avg Epsilon: 0.0110\n",
            "Vanilla DQN | Episode 466, Reward: 110.0, Avg Epsilon: 0.0110\n",
            "Vanilla DQN | Episode 467, Reward: 175.0, Avg Epsilon: 0.0110\n",
            "Vanilla DQN | Episode 468, Reward: 235.0, Avg Epsilon: 0.0109\n",
            "Vanilla DQN | Episode 469, Reward: 100.0, Avg Epsilon: 0.0109\n",
            "Vanilla DQN | Episode 470, Reward: 50.0, Avg Epsilon: 0.0109\n",
            "Vanilla DQN | Episode 471, Reward: 310.0, Avg Epsilon: 0.0109\n",
            "Vanilla DQN | Episode 472, Reward: 110.0, Avg Epsilon: 0.0109\n",
            "Vanilla DQN | Episode 473, Reward: 100.0, Avg Epsilon: 0.0109\n",
            "Vanilla DQN | Episode 474, Reward: 130.0, Avg Epsilon: 0.0109\n",
            "Vanilla DQN | Episode 475, Reward: 90.0, Avg Epsilon: 0.0109\n",
            "Vanilla DQN | Episode 476, Reward: 140.0, Avg Epsilon: 0.0109\n",
            "Vanilla DQN | Episode 477, Reward: 100.0, Avg Epsilon: 0.0109\n",
            "Vanilla DQN | Episode 478, Reward: 70.0, Avg Epsilon: 0.0108\n",
            "Vanilla DQN | Episode 479, Reward: 100.0, Avg Epsilon: 0.0108\n",
            "Vanilla DQN | Episode 480, Reward: 130.0, Avg Epsilon: 0.0108\n",
            "Vanilla DQN | Episode 481, Reward: 110.0, Avg Epsilon: 0.0108\n",
            "Vanilla DQN | Episode 482, Reward: 280.0, Avg Epsilon: 0.0108\n",
            "Vanilla DQN | Episode 483, Reward: 80.0, Avg Epsilon: 0.0108\n",
            "Vanilla DQN | Episode 484, Reward: 150.0, Avg Epsilon: 0.0108\n",
            "Vanilla DQN | Episode 485, Reward: 80.0, Avg Epsilon: 0.0108\n",
            "Vanilla DQN | Episode 486, Reward: 130.0, Avg Epsilon: 0.0107\n",
            "Vanilla DQN | Episode 487, Reward: 160.0, Avg Epsilon: 0.0107\n",
            "Vanilla DQN | Episode 488, Reward: 130.0, Avg Epsilon: 0.0107\n",
            "Vanilla DQN | Episode 489, Reward: 100.0, Avg Epsilon: 0.0107\n",
            "Vanilla DQN | Episode 490, Reward: 160.0, Avg Epsilon: 0.0107\n",
            "Vanilla DQN | Episode 491, Reward: 205.0, Avg Epsilon: 0.0107\n",
            "Vanilla DQN | Episode 492, Reward: 130.0, Avg Epsilon: 0.0107\n",
            "Vanilla DQN | Episode 493, Reward: 295.0, Avg Epsilon: 0.0107\n",
            "Vanilla DQN | Episode 494, Reward: 120.0, Avg Epsilon: 0.0107\n",
            "Vanilla DQN | Episode 495, Reward: 70.0, Avg Epsilon: 0.0107\n",
            "Vanilla DQN | Episode 496, Reward: 325.0, Avg Epsilon: 0.0106\n",
            "Vanilla DQN | Episode 497, Reward: 120.0, Avg Epsilon: 0.0106\n",
            "Vanilla DQN | Episode 498, Reward: 80.0, Avg Epsilon: 0.0106\n",
            "Vanilla DQN | Episode 499, Reward: 80.0, Avg Epsilon: 0.0106\n",
            "Vanilla DQN | Episode 500, Reward: 190.0, Avg Epsilon: 0.0106\n",
            "Vanilla DQN | Episode 500, Loaded best model from vanilla_best.pt\n",
            "Vanilla DQN | Episode 501, Reward: 90.0, Avg Epsilon: 0.0106\n",
            "Vanilla DQN | Episode 502, Reward: 160.0, Avg Epsilon: 0.0106\n",
            "Vanilla DQN | Episode 503, Reward: 160.0, Avg Epsilon: 0.0106\n",
            "Vanilla DQN | Episode 504, Reward: 90.0, Avg Epsilon: 0.0106\n",
            "Vanilla DQN | Episode 505, Reward: 160.0, Avg Epsilon: 0.0106\n",
            "Vanilla DQN | Episode 506, Reward: 50.0, Avg Epsilon: 0.0106\n",
            "Vanilla DQN | Episode 507, Reward: 235.0, Avg Epsilon: 0.0106\n",
            "Vanilla DQN | Episode 508, Reward: 90.0, Avg Epsilon: 0.0106\n",
            "Vanilla DQN | Episode 509, Reward: 190.0, Avg Epsilon: 0.0105\n",
            "Vanilla DQN | Episode 510, Reward: 130.0, Avg Epsilon: 0.0105\n",
            "Vanilla DQN | Episode 511, Reward: 90.0, Avg Epsilon: 0.0105\n",
            "Vanilla DQN | Episode 512, Reward: 60.0, Avg Epsilon: 0.0105\n",
            "Vanilla DQN | Episode 513, Reward: 130.0, Avg Epsilon: 0.0105\n",
            "Vanilla DQN | Episode 514, Reward: 160.0, Avg Epsilon: 0.0105\n",
            "Vanilla DQN | Episode 515, Reward: 90.0, Avg Epsilon: 0.0105\n",
            "Vanilla DQN | Episode 516, Reward: 20.0, Avg Epsilon: 0.0105\n",
            "Vanilla DQN | Episode 517, Reward: 140.0, Avg Epsilon: 0.0105\n",
            "Vanilla DQN | Episode 518, Reward: 40.0, Avg Epsilon: 0.0105\n",
            "Vanilla DQN | Episode 519, Reward: 110.0, Avg Epsilon: 0.0105\n",
            "Vanilla DQN | Episode 520, Reward: 50.0, Avg Epsilon: 0.0105\n",
            "Vanilla DQN | Episode 521, Reward: 60.0, Avg Epsilon: 0.0105\n",
            "Vanilla DQN | Episode 522, Reward: 60.0, Avg Epsilon: 0.0105\n",
            "Vanilla DQN | Episode 523, Reward: 120.0, Avg Epsilon: 0.0105\n",
            "Vanilla DQN | Episode 524, Reward: 175.0, Avg Epsilon: 0.0105\n",
            "Vanilla DQN | Episode 525, Reward: 90.0, Avg Epsilon: 0.0105\n",
            "Vanilla DQN | Episode 526, Reward: 120.0, Avg Epsilon: 0.0105\n",
            "Vanilla DQN | Episode 527, Reward: 40.0, Avg Epsilon: 0.0105\n",
            "Vanilla DQN | Episode 528, Reward: 120.0, Avg Epsilon: 0.0104\n",
            "Vanilla DQN | Episode 529, Reward: 150.0, Avg Epsilon: 0.0104\n",
            "Vanilla DQN | Episode 530, Reward: 60.0, Avg Epsilon: 0.0104\n",
            "Vanilla DQN | Episode 531, Reward: 130.0, Avg Epsilon: 0.0104\n",
            "Vanilla DQN | Episode 532, Reward: 150.0, Avg Epsilon: 0.0104\n",
            "Vanilla DQN | Episode 533, Reward: 50.0, Avg Epsilon: 0.0104\n",
            "Vanilla DQN | Episode 534, Reward: 50.0, Avg Epsilon: 0.0104\n",
            "Vanilla DQN | Episode 535, Reward: 150.0, Avg Epsilon: 0.0104\n",
            "Vanilla DQN | Episode 536, Reward: 220.0, Avg Epsilon: 0.0104\n",
            "Vanilla DQN | Episode 537, Reward: 130.0, Avg Epsilon: 0.0104\n",
            "Vanilla DQN | Episode 538, Reward: 340.0, Avg Epsilon: 0.0104\n",
            "Vanilla DQN | Episode 539, Reward: 70.0, Avg Epsilon: 0.0104\n",
            "Vanilla DQN | Episode 540, Reward: 90.0, Avg Epsilon: 0.0104\n",
            "Vanilla DQN | Episode 541, Reward: 130.0, Avg Epsilon: 0.0104\n",
            "Vanilla DQN | Episode 542, Reward: 20.0, Avg Epsilon: 0.0104\n",
            "Vanilla DQN | Episode 543, Reward: 140.0, Avg Epsilon: 0.0104\n",
            "Vanilla DQN | Episode 544, Reward: 295.0, Avg Epsilon: 0.0103\n",
            "Vanilla DQN | Episode 545, Reward: 80.0, Avg Epsilon: 0.0103\n",
            "Vanilla DQN | Episode 546, Reward: 280.0, Avg Epsilon: 0.0103\n",
            "Vanilla DQN | Episode 547, Reward: 70.0, Avg Epsilon: 0.0103\n",
            "Vanilla DQN | Episode 548, Reward: 50.0, Avg Epsilon: 0.0103\n",
            "Vanilla DQN | Episode 549, Reward: 220.0, Avg Epsilon: 0.0103\n",
            "Vanilla DQN | Episode 550, Reward: 100.0, Avg Epsilon: 0.0103\n",
            "Vanilla DQN | Episode 550, Loaded best model from vanilla_best.pt\n",
            "Vanilla DQN | Episode 551, Reward: 150.0, Avg Epsilon: 0.0103\n",
            "Vanilla DQN | Episode 552, Reward: 100.0, Avg Epsilon: 0.0103\n",
            "Vanilla DQN | Episode 553, Reward: 70.0, Avg Epsilon: 0.0103\n",
            "Vanilla DQN | Episode 554, Reward: 90.0, Avg Epsilon: 0.0103\n",
            "Vanilla DQN | Episode 555, Reward: 140.0, Avg Epsilon: 0.0103\n",
            "Vanilla DQN | Episode 556, Reward: 50.0, Avg Epsilon: 0.0103\n",
            "Vanilla DQN | Episode 557, Reward: 160.0, Avg Epsilon: 0.0103\n",
            "Vanilla DQN | Episode 558, Reward: 60.0, Avg Epsilon: 0.0103\n",
            "Vanilla DQN | Episode 559, Reward: 130.0, Avg Epsilon: 0.0103\n",
            "Vanilla DQN | Episode 560, Reward: 190.0, Avg Epsilon: 0.0103\n",
            "Vanilla DQN | Episode 561, Reward: 80.0, Avg Epsilon: 0.0103\n",
            "Vanilla DQN | Episode 562, Reward: 190.0, Avg Epsilon: 0.0103\n",
            "Vanilla DQN | Episode 563, Reward: 140.0, Avg Epsilon: 0.0103\n",
            "Vanilla DQN | Episode 564, Reward: 120.0, Avg Epsilon: 0.0103\n",
            "Vanilla DQN | Episode 565, Reward: 160.0, Avg Epsilon: 0.0103\n",
            "Vanilla DQN | Episode 566, Reward: 10.0, Avg Epsilon: 0.0103\n",
            "Vanilla DQN | Episode 567, Reward: 50.0, Avg Epsilon: 0.0103\n",
            "Vanilla DQN | Episode 568, Reward: 100.0, Avg Epsilon: 0.0102\n",
            "Vanilla DQN | Episode 569, Reward: 150.0, Avg Epsilon: 0.0102\n",
            "Vanilla DQN | Episode 570, Reward: 310.0, Avg Epsilon: 0.0102\n",
            "Vanilla DQN | Episode 571, Reward: 100.0, Avg Epsilon: 0.0102\n",
            "Vanilla DQN | Episode 572, Reward: 20.0, Avg Epsilon: 0.0102\n",
            "Vanilla DQN | Episode 573, Reward: 175.0, Avg Epsilon: 0.0102\n",
            "Vanilla DQN | Episode 574, Reward: 90.0, Avg Epsilon: 0.0102\n",
            "Vanilla DQN | Episode 575, Reward: 90.0, Avg Epsilon: 0.0102\n",
            "Vanilla DQN | Episode 576, Reward: 140.0, Avg Epsilon: 0.0102\n",
            "Vanilla DQN | Episode 577, Reward: 80.0, Avg Epsilon: 0.0102\n",
            "Vanilla DQN | Episode 578, Reward: 50.0, Avg Epsilon: 0.0102\n",
            "Vanilla DQN | Episode 579, Reward: 40.0, Avg Epsilon: 0.0102\n",
            "Vanilla DQN | Episode 580, Reward: 60.0, Avg Epsilon: 0.0102\n",
            "Vanilla DQN | Episode 581, Reward: 190.0, Avg Epsilon: 0.0102\n",
            "Vanilla DQN | Episode 582, Reward: 80.0, Avg Epsilon: 0.0102\n",
            "Vanilla DQN | Episode 583, Reward: 220.0, Avg Epsilon: 0.0102\n",
            "Vanilla DQN | Episode 584, Reward: 110.0, Avg Epsilon: 0.0102\n",
            "Vanilla DQN | Episode 585, Reward: 130.0, Avg Epsilon: 0.0102\n",
            "Vanilla DQN | Episode 586, Reward: 150.0, Avg Epsilon: 0.0102\n",
            "Vanilla DQN | Episode 587, Reward: 90.0, Avg Epsilon: 0.0102\n",
            "Vanilla DQN | Episode 588, Reward: 160.0, Avg Epsilon: 0.0102\n",
            "Vanilla DQN | Episode 589, Reward: 110.0, Avg Epsilon: 0.0102\n",
            "Vanilla DQN | Episode 590, Reward: 100.0, Avg Epsilon: 0.0102\n",
            "Vanilla DQN | Episode 591, Reward: 80.0, Avg Epsilon: 0.0102\n",
            "Vanilla DQN | Episode 592, Reward: 60.0, Avg Epsilon: 0.0102\n",
            "Vanilla DQN | Episode 593, Reward: 130.0, Avg Epsilon: 0.0102\n",
            "Vanilla DQN | Episode 594, Reward: 150.0, Avg Epsilon: 0.0102\n",
            "Vanilla DQN | Episode 595, Reward: 130.0, Avg Epsilon: 0.0102\n",
            "Vanilla DQN | Episode 596, Reward: 110.0, Avg Epsilon: 0.0102\n",
            "Vanilla DQN | Episode 597, Reward: 130.0, Avg Epsilon: 0.0102\n",
            "Vanilla DQN | Episode 598, Reward: 90.0, Avg Epsilon: 0.0102\n",
            "Vanilla DQN | Episode 599, Reward: 110.0, Avg Epsilon: 0.0102\n",
            "Vanilla DQN | Episode 600, Reward: 90.0, Avg Epsilon: 0.0102\n",
            "Vanilla DQN | Episode 600, Loaded best model from vanilla_best.pt\n",
            "Vanilla DQN | Episode 601, Reward: 295.0, Avg Epsilon: 0.0102\n",
            "Vanilla DQN | Episode 602, Reward: 80.0, Avg Epsilon: 0.0102\n",
            "Vanilla DQN | Episode 603, Reward: 80.0, Avg Epsilon: 0.0102\n",
            "Vanilla DQN | Episode 604, Reward: 40.0, Avg Epsilon: 0.0102\n",
            "Vanilla DQN | Episode 605, Reward: 80.0, Avg Epsilon: 0.0102\n",
            "Vanilla DQN | Episode 606, Reward: 30.0, Avg Epsilon: 0.0102\n",
            "Vanilla DQN | Episode 607, Reward: 205.0, Avg Epsilon: 0.0102\n",
            "Vanilla DQN | Episode 608, Reward: 100.0, Avg Epsilon: 0.0101\n",
            "Vanilla DQN | Episode 609, Reward: 130.0, Avg Epsilon: 0.0101\n",
            "Vanilla DQN | Episode 610, Reward: 60.0, Avg Epsilon: 0.0101\n",
            "Vanilla DQN | Episode 611, Reward: 120.0, Avg Epsilon: 0.0101\n",
            "Vanilla DQN | Episode 612, Reward: 80.0, Avg Epsilon: 0.0101\n",
            "Vanilla DQN | Episode 613, Reward: 70.0, Avg Epsilon: 0.0101\n",
            "Vanilla DQN | Episode 614, Reward: 130.0, Avg Epsilon: 0.0101\n",
            "Vanilla DQN | Episode 615, Reward: 265.0, Avg Epsilon: 0.0101\n",
            "Vanilla DQN | Episode 616, Reward: 130.0, Avg Epsilon: 0.0101\n",
            "Vanilla DQN | Episode 617, Reward: 120.0, Avg Epsilon: 0.0101\n",
            "Vanilla DQN | Episode 618, Reward: 205.0, Avg Epsilon: 0.0101\n",
            "Vanilla DQN | Episode 619, Reward: 100.0, Avg Epsilon: 0.0101\n",
            "Vanilla DQN | Episode 620, Reward: 90.0, Avg Epsilon: 0.0101\n",
            "Vanilla DQN | Episode 621, Reward: 160.0, Avg Epsilon: 0.0101\n",
            "Vanilla DQN | Episode 622, Reward: 80.0, Avg Epsilon: 0.0101\n",
            "Vanilla DQN | Episode 623, Reward: 70.0, Avg Epsilon: 0.0101\n",
            "Vanilla DQN | Episode 624, Reward: 110.0, Avg Epsilon: 0.0101\n",
            "Vanilla DQN | Episode 625, Reward: 130.0, Avg Epsilon: 0.0101\n",
            "Vanilla DQN | Episode 626, Reward: 175.0, Avg Epsilon: 0.0101\n",
            "Vanilla DQN | Episode 627, Reward: 190.0, Avg Epsilon: 0.0101\n",
            "Vanilla DQN | Episode 628, Reward: 175.0, Avg Epsilon: 0.0101\n",
            "Vanilla DQN | Episode 629, Reward: 190.0, Avg Epsilon: 0.0101\n",
            "Vanilla DQN | Episode 630, Reward: 80.0, Avg Epsilon: 0.0101\n",
            "Vanilla DQN | Episode 631, Reward: 50.0, Avg Epsilon: 0.0101\n",
            "Vanilla DQN | Episode 632, Reward: 90.0, Avg Epsilon: 0.0101\n",
            "Vanilla DQN | Episode 633, Reward: 120.0, Avg Epsilon: 0.0101\n",
            "Vanilla DQN | Episode 634, Reward: 100.0, Avg Epsilon: 0.0101\n",
            "Vanilla DQN | Episode 635, Reward: 80.0, Avg Epsilon: 0.0101\n",
            "Vanilla DQN | Episode 636, Reward: 340.0, Avg Epsilon: 0.0101\n",
            "Vanilla DQN | Episode 637, Reward: 80.0, Avg Epsilon: 0.0101\n",
            "Vanilla DQN | Episode 638, Reward: 40.0, Avg Epsilon: 0.0101\n",
            "Vanilla DQN | Episode 639, Reward: 40.0, Avg Epsilon: 0.0101\n",
            "Vanilla DQN | Episode 640, Reward: 120.0, Avg Epsilon: 0.0101\n",
            "Vanilla DQN | Episode 641, Reward: 30.0, Avg Epsilon: 0.0101\n",
            "Vanilla DQN | Episode 642, Reward: 220.0, Avg Epsilon: 0.0101\n",
            "Vanilla DQN | Episode 643, Reward: 80.0, Avg Epsilon: 0.0101\n",
            "Vanilla DQN | Episode 644, Reward: 90.0, Avg Epsilon: 0.0101\n",
            "Vanilla DQN | Episode 645, Reward: 175.0, Avg Epsilon: 0.0101\n",
            "Vanilla DQN | Episode 646, Reward: 150.0, Avg Epsilon: 0.0101\n",
            "Vanilla DQN | Episode 647, Reward: 100.0, Avg Epsilon: 0.0101\n",
            "Vanilla DQN | Episode 648, Reward: 60.0, Avg Epsilon: 0.0101\n",
            "Vanilla DQN | Episode 649, Reward: 100.0, Avg Epsilon: 0.0101\n",
            "Vanilla DQN | Episode 650, Reward: 175.0, Avg Epsilon: 0.0101\n",
            "Vanilla DQN | Episode 650, Loaded best model from vanilla_best.pt\n",
            "Vanilla DQN | Episode 651, Reward: 50.0, Avg Epsilon: 0.0101\n",
            "Vanilla DQN | Episode 652, Reward: 110.0, Avg Epsilon: 0.0101\n",
            "Vanilla DQN | Episode 653, Reward: 80.0, Avg Epsilon: 0.0101\n",
            "Vanilla DQN | Episode 654, Reward: 110.0, Avg Epsilon: 0.0101\n",
            "Vanilla DQN | Episode 655, Reward: 80.0, Avg Epsilon: 0.0101\n",
            "Vanilla DQN | Episode 656, Reward: 110.0, Avg Epsilon: 0.0101\n",
            "Vanilla DQN | Episode 657, Reward: 130.0, Avg Epsilon: 0.0101\n",
            "Vanilla DQN | Episode 658, Reward: 280.0, Avg Epsilon: 0.0101\n",
            "Vanilla DQN | Episode 659, Reward: 70.0, Avg Epsilon: 0.0101\n",
            "Vanilla DQN | Episode 660, Reward: 110.0, Avg Epsilon: 0.0101\n",
            "Vanilla DQN | Episode 661, Reward: 40.0, Avg Epsilon: 0.0101\n",
            "Vanilla DQN | Episode 662, Reward: 60.0, Avg Epsilon: 0.0101\n",
            "Vanilla DQN | Episode 663, Reward: 70.0, Avg Epsilon: 0.0101\n",
            "Vanilla DQN | Episode 664, Reward: 90.0, Avg Epsilon: 0.0101\n",
            "Vanilla DQN | Episode 665, Reward: 220.0, Avg Epsilon: 0.0101\n",
            "Vanilla DQN | Episode 666, Reward: 340.0, Avg Epsilon: 0.0101\n",
            "Vanilla DQN | Episode 667, Reward: 60.0, Avg Epsilon: 0.0101\n",
            "Vanilla DQN | Episode 668, Reward: 40.0, Avg Epsilon: 0.0101\n",
            "Vanilla DQN | Episode 669, Reward: 190.0, Avg Epsilon: 0.0101\n",
            "Vanilla DQN | Episode 670, Reward: 235.0, Avg Epsilon: 0.0101\n",
            "Vanilla DQN | Episode 671, Reward: 130.0, Avg Epsilon: 0.0101\n",
            "Vanilla DQN | Episode 672, Reward: 190.0, Avg Epsilon: 0.0101\n",
            "Vanilla DQN | Episode 673, Reward: 160.0, Avg Epsilon: 0.0101\n",
            "Vanilla DQN | Episode 674, Reward: 120.0, Avg Epsilon: 0.0101\n",
            "Vanilla DQN | Episode 675, Reward: 60.0, Avg Epsilon: 0.0101\n",
            "Vanilla DQN | Episode 676, Reward: 50.0, Avg Epsilon: 0.0101\n",
            "Vanilla DQN | Episode 677, Reward: 190.0, Avg Epsilon: 0.0101\n",
            "Vanilla DQN | Episode 678, Reward: 60.0, Avg Epsilon: 0.0101\n",
            "Vanilla DQN | Episode 679, Reward: 90.0, Avg Epsilon: 0.0101\n",
            "Vanilla DQN | Episode 680, Reward: 265.0, Avg Epsilon: 0.0101\n",
            "Vanilla DQN | Episode 681, Reward: 60.0, Avg Epsilon: 0.0101\n",
            "Vanilla DQN | Episode 682, Reward: 220.0, Avg Epsilon: 0.0101\n",
            "Vanilla DQN | Episode 683, Reward: 190.0, Avg Epsilon: 0.0101\n",
            "Vanilla DQN | Episode 684, Reward: 50.0, Avg Epsilon: 0.0101\n",
            "Vanilla DQN | Episode 685, Reward: 190.0, Avg Epsilon: 0.0101\n",
            "Vanilla DQN | Episode 686, Reward: 110.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 687, Reward: 250.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 688, Reward: 40.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 689, Reward: 295.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 690, Reward: 100.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 691, Reward: 325.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 692, Reward: 70.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 693, Reward: 120.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 694, Reward: 70.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 695, Reward: 295.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 696, Reward: 110.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 697, Reward: 120.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 698, Reward: 100.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 699, Reward: 50.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 700, Reward: 100.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 700, Loaded best model from vanilla_best.pt\n",
            "Vanilla DQN | Episode 701, Reward: 40.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 702, Reward: 110.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 703, Reward: 140.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 704, Reward: 160.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 705, Reward: 190.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 706, Reward: 90.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 707, Reward: 235.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 708, Reward: 100.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 709, Reward: 90.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 710, Reward: 120.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 711, Reward: 90.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 712, Reward: 250.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 713, Reward: 60.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 714, Reward: 120.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 715, Reward: 110.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 716, Reward: 50.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 717, Reward: 100.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 718, Reward: 110.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 719, Reward: 90.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 720, Reward: 50.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 721, Reward: 110.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 722, Reward: 50.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 723, Reward: 150.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 724, Reward: 60.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 725, Reward: 60.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 726, Reward: 150.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 727, Reward: 235.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 728, Reward: 60.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 729, Reward: 40.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 730, Reward: 90.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 731, Reward: 265.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 732, Reward: 60.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 733, Reward: 50.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 734, Reward: 90.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 735, Reward: 60.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 736, Reward: 90.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 737, Reward: 90.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 738, Reward: 190.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 739, Reward: 120.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 740, Reward: 60.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 741, Reward: 130.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 742, Reward: 100.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 743, Reward: 80.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 744, Reward: 160.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 745, Reward: 190.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 746, Reward: 205.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 747, Reward: 50.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 748, Reward: 370.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 749, Reward: 30.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 750, Reward: 190.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 750, Loaded best model from vanilla_best.pt\n",
            "Vanilla DQN | Episode 751, Reward: 40.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 752, Reward: 60.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 753, Reward: 30.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 754, Reward: 100.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 755, Reward: 90.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 756, Reward: 120.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 757, Reward: 60.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 758, Reward: 175.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 759, Reward: 310.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 760, Reward: 340.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 761, Reward: 110.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 762, Reward: 50.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 763, Reward: 120.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 764, Reward: 90.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 765, Reward: 160.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 766, Reward: 60.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 767, Reward: 100.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 768, Reward: 175.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 769, Reward: 140.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 770, Reward: 110.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 771, Reward: 250.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 772, Reward: 220.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 773, Reward: 130.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 774, Reward: 80.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 775, Reward: 190.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 776, Reward: 235.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 777, Reward: 40.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 778, Reward: 120.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 779, Reward: 355.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 780, Reward: 190.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 781, Reward: 160.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 782, Reward: 100.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 783, Reward: 220.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 784, Reward: 60.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 785, Reward: 30.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 786, Reward: 175.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 787, Reward: 160.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 788, Reward: 175.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 789, Reward: 100.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 790, Reward: 80.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 791, Reward: 130.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 792, Reward: 130.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 793, Reward: 220.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 794, Reward: 160.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 795, Reward: 110.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 796, Reward: 100.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 797, Reward: 235.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 798, Reward: 80.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 799, Reward: 140.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 800, Reward: 40.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 800, Loaded best model from vanilla_best.pt\n",
            "Vanilla DQN | Episode 801, Reward: 100.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 802, Reward: 140.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 803, Reward: 60.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 804, Reward: 70.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 805, Reward: 130.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 806, Reward: 385.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 807, Reward: 70.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 808, Reward: 80.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 809, Reward: 235.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 810, Reward: 100.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 811, Reward: 60.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 812, Reward: 160.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 813, Reward: 175.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 814, Reward: 70.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 815, Reward: 70.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 816, Reward: 130.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 817, Reward: 220.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 818, Reward: 130.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 819, Reward: 100.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 820, Reward: 160.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 821, Reward: 120.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 822, Reward: 50.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 823, Reward: 175.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 824, Reward: 150.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 825, Reward: 80.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 826, Reward: 100.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 827, Reward: 80.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 828, Reward: 90.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 829, Reward: 130.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 830, Reward: 60.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 831, Reward: 100.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 832, Reward: 80.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 833, Reward: 70.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 834, Reward: 205.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 835, Reward: 100.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 836, Reward: 175.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 837, Reward: 220.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 838, Reward: 100.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 839, Reward: 90.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 840, Reward: 90.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 841, Reward: 120.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 842, Reward: 90.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 843, Reward: 130.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 844, Reward: 130.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 845, Reward: 160.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 846, Reward: 110.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 847, Reward: 30.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 848, Reward: 100.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 849, Reward: 80.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 850, Reward: 90.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 850, Loaded best model from vanilla_best.pt\n",
            "Vanilla DQN | Episode 851, Reward: 190.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 852, Reward: 190.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 853, Reward: 120.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 854, Reward: 190.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 855, Reward: 50.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 856, Reward: 140.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 857, Reward: 355.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 858, Reward: 120.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 859, Reward: 130.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 860, Reward: 205.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 861, Reward: 205.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 862, Reward: 130.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 863, Reward: 130.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 864, Reward: 90.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 865, Reward: 50.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 866, Reward: 100.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 867, Reward: 190.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 868, Reward: 100.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 869, Reward: 50.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 870, Reward: 220.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 871, Reward: 160.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 872, Reward: 190.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 873, Reward: 40.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 874, Reward: 140.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 875, Reward: 40.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 876, Reward: 100.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 877, Reward: 150.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 878, Reward: 250.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 879, Reward: 60.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 880, Reward: 130.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 881, Reward: 325.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 882, Reward: 110.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 883, Reward: 130.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 884, Reward: 110.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 885, Reward: 130.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 886, Reward: 235.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 887, Reward: 40.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 888, Reward: 80.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 889, Reward: 140.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 890, Reward: 90.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 891, Reward: 140.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 892, Reward: 60.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 893, Reward: 120.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 894, Reward: 130.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 895, Reward: 100.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 896, Reward: 30.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 897, Reward: 90.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 898, Reward: 160.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 899, Reward: 175.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 900, Reward: 140.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 900, Loaded best model from vanilla_best.pt\n",
            "Vanilla DQN | Episode 901, Reward: 205.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 902, Reward: 140.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 903, Reward: 160.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 904, Reward: 80.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 905, Reward: 80.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 906, Reward: 120.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 907, Reward: 70.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 908, Reward: 30.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 909, Reward: 90.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 910, Reward: 90.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 911, Reward: 70.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 912, Reward: 60.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 913, Reward: 60.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 914, Reward: 30.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 915, Reward: 120.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 916, Reward: 80.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 917, Reward: 50.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 918, Reward: 60.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 919, Reward: 70.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 920, Reward: 130.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 921, Reward: 140.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 922, Reward: 130.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 923, Reward: 40.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 924, Reward: 40.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 925, Reward: 30.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 926, Reward: 80.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 927, Reward: 110.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 928, Reward: 90.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 929, Reward: 120.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 930, Reward: 60.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 931, Reward: 50.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 932, Reward: 175.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 933, Reward: 30.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 934, Reward: 30.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 935, Reward: 110.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 936, Reward: 100.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 937, Reward: 130.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 938, Reward: 190.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 939, Reward: 175.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 940, Reward: 110.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 941, Reward: 140.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 942, Reward: 110.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 943, Reward: 100.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 944, Reward: 175.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 945, Reward: 120.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 946, Reward: 90.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 947, Reward: 50.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 948, Reward: 325.0, Avg Epsilon: 0.0100\n",
            "Vanilla DQN | Episode 949, Reward: 150.0, Avg Epsilon: 0.0100\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "cuDNN error: CUDNN_STATUS_EXECUTION_FAILED",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[17], line 59\u001b[0m\n\u001b[0;32m     55\u001b[0m             torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mempty_cache()\n\u001b[0;32m     57\u001b[0m     \u001b[39mreturn\u001b[39;00m all_rewards, epsilon_values\n\u001b[1;32m---> 59\u001b[0m vanilla_rewards, vanilla_epsilons \u001b[39m=\u001b[39m train_vanilla_dqn(\n\u001b[0;32m     60\u001b[0m     env\u001b[39m=\u001b[39;49menv, num_episodes\u001b[39m=\u001b[39;49mnum_episodes, device\u001b[39m=\u001b[39;49mdevice,\n\u001b[0;32m     61\u001b[0m     epsilon_start\u001b[39m=\u001b[39;49mepsilon_start, epsilon_end\u001b[39m=\u001b[39;49mepsilon_end, epsilon_decay\u001b[39m=\u001b[39;49mepsilon_decay,\n\u001b[0;32m     62\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size, gamma\u001b[39m=\u001b[39;49mgamma, update_target_every\u001b[39m=\u001b[39;49mupdate_target_every,\n\u001b[0;32m     63\u001b[0m     memory_capacity\u001b[39m=\u001b[39;49mmemory_capacity, lr\u001b[39m=\u001b[39;49mlr\n\u001b[0;32m     64\u001b[0m )\n",
            "Cell \u001b[1;32mIn[17], line 36\u001b[0m, in \u001b[0;36mtrain_vanilla_dqn\u001b[1;34m(env, num_episodes, device, epsilon_start, epsilon_end, epsilon_decay, batch_size, gamma, update_target_every, memory_capacity, lr, save_path)\u001b[0m\n\u001b[0;32m     33\u001b[0m total_reward \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m reward\n\u001b[0;32m     34\u001b[0m steps_done \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m---> 36\u001b[0m train(memory, policy_net, target_net, optimizer, batch_size, gamma, use_double_dqn\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, device\u001b[39m=\u001b[39;49mdevice)\n\u001b[0;32m     38\u001b[0m \u001b[39mif\u001b[39;00m steps_done \u001b[39m%\u001b[39m update_target_every \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     39\u001b[0m     target_net\u001b[39m.\u001b[39mload_state_dict(policy_net\u001b[39m.\u001b[39mstate_dict())\n",
            "Cell \u001b[1;32mIn[12], line 29\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(memory, policy_net, target_net, optimizer, batch_size, gamma, use_double_dqn, device)\u001b[0m\n\u001b[0;32m     27\u001b[0m         next_q_values \u001b[39m=\u001b[39m target_net(next_states)\u001b[39m.\u001b[39mgather(\u001b[39m1\u001b[39m, next_actions)\n\u001b[0;32m     28\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 29\u001b[0m         next_q_values \u001b[39m=\u001b[39m target_net(next_states)\u001b[39m.\u001b[39mmax(\u001b[39m1\u001b[39m)[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\n\u001b[0;32m     31\u001b[0m     expected_q_values \u001b[39m=\u001b[39m rewards \u001b[39m+\u001b[39m (gamma \u001b[39m*\u001b[39m next_q_values \u001b[39m*\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m dones))\n\u001b[0;32m     33\u001b[0m loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmse_loss(q_values, expected_q_values)\n",
            "File \u001b[1;32mc:\\Users\\aayus\\DQN\\.conda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\aayus\\DQN\\.conda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[11], line 19\u001b[0m, in \u001b[0;36mDQN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> 19\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnet(x)\n",
            "File \u001b[1;32mc:\\Users\\aayus\\DQN\\.conda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\aayus\\DQN\\.conda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\aayus\\DQN\\.conda\\lib\\site-packages\\torch\\nn\\modules\\container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    218\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 219\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    220\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\aayus\\DQN\\.conda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\aayus\\DQN\\.conda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\aayus\\DQN\\.conda\\lib\\site-packages\\torch\\nn\\modules\\conv.py:458\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    457\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 458\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
            "File \u001b[1;32mc:\\Users\\aayus\\DQN\\.conda\\lib\\site-packages\\torch\\nn\\modules\\conv.py:454\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    451\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    452\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    453\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> 454\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    455\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
            "\u001b[1;31mRuntimeError\u001b[0m: cuDNN error: CUDNN_STATUS_EXECUTION_FAILED"
          ]
        }
      ],
      "source": [
        "#Vanilla DQN\n",
        "def train_vanilla_dqn(env, num_episodes, device, epsilon_start, epsilon_end, epsilon_decay, \n",
        "                      batch_size, gamma, update_target_every, memory_capacity, lr, save_path='vanilla_best.pt'):\n",
        "    policy_net = DQN(env.action_space.n).to(device)\n",
        "    target_net = DQN(env.action_space.n).to(device)\n",
        "    target_net.load_state_dict(policy_net.state_dict())\n",
        "    optimizer = optim.Adam(policy_net.parameters(), lr=lr)\n",
        "    memory = ReplayBuffer(memory_capacity)\n",
        "\n",
        "    all_rewards = []\n",
        "    epsilon_values = []\n",
        "    steps_done = 0\n",
        "    best_reward = -float('inf')\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        obs = env.reset()[0] if isinstance(env.reset(), tuple) else env.reset()\n",
        "        state, stacked_frames = stack_frames(None, obs, True)\n",
        "        total_reward = 0\n",
        "        episode_epsilon = []\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            env.render()\n",
        "            epsilon = epsilon_end + (epsilon_start - epsilon_end) * np.exp(-1. * steps_done / epsilon_decay)\n",
        "            episode_epsilon.append(epsilon)\n",
        "            action = select_action(state, steps_done, epsilon, policy_net, device, env)\n",
        "            next_obs, reward, terminated, truncated, _ = env.step(action)\n",
        "            next_state, stacked_frames = stack_frames(stacked_frames, next_obs, False)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            memory.push(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "            steps_done += 1\n",
        "\n",
        "            train(memory, policy_net, target_net, optimizer, batch_size, gamma, use_double_dqn=False, device=device)\n",
        "\n",
        "            if steps_done % update_target_every == 0:\n",
        "                target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "        all_rewards.append(total_reward)\n",
        "        epsilon_values.append(np.mean(episode_epsilon))\n",
        "        print(f\"Vanilla DQN | Episode {episode}, Reward: {total_reward}, Avg Epsilon: {epsilon_values[-1]:.4f}\")\n",
        "\n",
        "        if total_reward > best_reward:\n",
        "            best_reward = total_reward\n",
        "            torch.save(policy_net.state_dict(), save_path)\n",
        "\n",
        "        if episode % 50 == 0 and episode > 0:\n",
        "            policy_net.load_state_dict(torch.load(save_path))\n",
        "            target_net.load_state_dict(torch.load(save_path))\n",
        "            print(f\"Vanilla DQN | Episode {episode}, Loaded best model from {save_path}\")\n",
        "\n",
        "        if device.type == \"cuda\":\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    return all_rewards, epsilon_values\n",
        "\n",
        "vanilla_rewards, vanilla_epsilons = train_vanilla_dqn(\n",
        "    env=env, num_episodes=num_episodes, device=device,\n",
        "    epsilon_start=epsilon_start, epsilon_end=epsilon_end, epsilon_decay=epsilon_decay,\n",
        "    batch_size=batch_size, gamma=gamma, update_target_every=update_target_every,\n",
        "    memory_capacity=memory_capacity, lr=lr\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_double_dqn(env, num_episodes, device, epsilon_start, epsilon_end, epsilon_decay, \n",
        "                     batch_size, gamma, update_target_every, memory_capacity, lr, save_path='double_best.pt'):\n",
        "    # Initialize networks\n",
        "    policy_net = DQN(env.action_space.n).to(device)\n",
        "    target_net = DQN(env.action_space.n).to(device)\n",
        "    target_net.load_state_dict(policy_net.state_dict())\n",
        "    optimizer = optim.Adam(policy_net.parameters(), lr=lr)\n",
        "    memory = ReplayBuffer(memory_capacity)\n",
        "    \n",
        "    all_rewards = []\n",
        "    epsilon_values = []\n",
        "    steps_done = 0\n",
        "    best_reward = -float('inf')\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        obs = env.reset()[0] if isinstance(env.reset(), tuple) else env.reset()\n",
        "        state, stacked_frames = stack_frames(None, obs, True)\n",
        "        total_reward = 0\n",
        "        episode_epsilon = []\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            env.render()\n",
        "            epsilon = epsilon_end + (epsilon_start - epsilon_end) * np.exp(-1. * steps_done / epsilon_decay)\n",
        "            episode_epsilon.append(epsilon)\n",
        "\n",
        "            action = select_action(state, steps_done, epsilon, policy_net, device, env)\n",
        "            next_obs, reward, terminated, truncated, _ = env.step(action)\n",
        "            next_state, stacked_frames = stack_frames(stacked_frames, next_obs, False)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            memory.push(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "            steps_done += 1\n",
        "\n",
        "            train(memory, policy_net, target_net, optimizer, batch_size, gamma, use_double_dqn=True, device=device)\n",
        "\n",
        "            if steps_done % update_target_every == 0:\n",
        "                target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "        all_rewards.append(total_reward)\n",
        "        epsilon_values.append(np.mean(episode_epsilon))\n",
        "        print(f\"Double DQN | Episode {episode}, Reward: {total_reward}, Avg Epsilon: {epsilon_values[-1]:}\")\n",
        "\n",
        "        if total_reward > best_reward:\n",
        "            best_reward = total_reward\n",
        "            torch.save(policy_net.state_dict(), save_path)\n",
        "\n",
        "        if episode % 50 == 0 and episode > 0:\n",
        "            policy_net.load_state_dict(torch.load(save_path))\n",
        "            target_net.load_state_dict(torch.load(save_path))\n",
        "            print(f\"Double DQN | Episode {episode}, Loaded best model from {save_path}\")\n",
        "\n",
        "        # Clear GPU memory after each episode\n",
        "        if device.type == \"cuda\":\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    return all_rewards, epsilon_values\n",
        "\n",
        "double_rewards, double_epsilons = train_double_dqn(\n",
        "    env=env, num_episodes=num_episodes, device=device,\n",
        "    epsilon_start=epsilon_start, epsilon_end=epsilon_end, epsilon_decay=epsilon_decay,\n",
        "    batch_size=batch_size, gamma=gamma, update_target_every=update_target_every,\n",
        "    memory_capacity=memory_capacity, lr=lr\n",
        ")\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(14, 6))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dyiq9-iC0qcK"
      },
      "outputs": [],
      "source": [
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(vanilla_rewards, label='Vanilla DQN', color='blue')\n",
        "plt.plot(double_rewards, label='Double DQN', color='orange')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Reward')\n",
        "plt.title('Training Rewards: Vanilla vs. Double DQN')\n",
        "plt.legend()\n",
        "plt.grid(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(vanilla_epsilons, label='Vanilla DQN', color='blue')\n",
        "plt.plot(double_epsilons, label='Double DQN', color='orange')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Average Epsilon')\n",
        "plt.title('Epsilon Decay: Vanilla vs. Double DQN')\n",
        "plt.legend()\n",
        "plt.grid(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
